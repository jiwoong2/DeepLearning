{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "1ucPW7jSERUk"
      ],
      "authorship_tag": "ABX9TyOPkgNk4OH3u5/7TwctGZv8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Gradient_Decent_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBlfzj0MD9Rx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pylab as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient 구현"
      ],
      "metadata": {
        "id": "1ucPW7jSERUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _numerical_gradient_no_batch(f, x): # 수치 gradient, 앞의 수치미분과 같은 개념\n",
        "    h = 1e-4 # 0.0001\n",
        "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성(이 경우 x와 같은 차원의 제로 벡터)\n",
        "\n",
        "    # gradient는 각 축에 대한 방향 미분을 모아놓은 벡터이므로 각 방향에 대한 수치미분을 grad 벡터에 차례로 대입하는 과정이다.\n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "\n",
        "        # f(x+h) 계산\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        # f(x-h) 계산\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "\n",
        "        grad[idx] =  (fxh1 - fxh2) / (2*h)\n",
        "        x[idx] = tmp_val # 값 복원\n",
        "\n",
        "    return grad"
      ],
      "metadata": {
        "id": "QXXudUg6ELhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(f, X): # 배치처리를 한 경우(행렬, 텐서 미분)\n",
        "    if X.ndim == 1: # 1차원, 그러니까 행렬로 묶이지 않은 벡터인 경우\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "\n",
        "        for idx, x in enumerate(X): # enumerate 설명1, 행벡터를 차례로 불러와 미분한다.\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "\n",
        "        return grad"
      ],
      "metadata": {
        "id": "MMfkglLnEW88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Decent 구현\n",
        "\n",
        "$ x_{n+1} = x_{n} - \\eta \\nabla f(x_{n}) $\n",
        "\n",
        "$\\eta = $ learning rate(학습률)"
      ],
      "metadata": {
        "id": "Wg45Y5qWEgiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_decent(f, init_x, lr = 0.01, step_num = 100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "\n",
        "    return x, np.array(x_history) #46/24"
      ],
      "metadata": {
        "id": "sV5m9Y4jEkqQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}