{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPEJmHPbC+EzjictZKZOAYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VZT9bJQiavL"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))"
      ],
      "metadata": {
        "id": "VmvyPiGti9KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_function(x):  # 항등 함수\n",
        "    return x"
      ],
      "metadata": {
        "id": "dSb3MSr0jK0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 인공신경망 구현\n",
        "\n",
        "아래 행렬식은 인공신경망의 첫번째 층을 수식화 한 것 이다. 노트 참고\n",
        "\\\n",
        "\\\n",
        "$x_{입력 노드,출력 노드}^{층}$\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "첫번째 층\n",
        "\\\n",
        "$(a_{1}^{(1)}, a_{2}^{(1)}, a_{3}^{(1)}) = (x_{1}, x_{2}) \\begin{pmatrix} w_{11}^{(1)} & w_{12}^{(1)} & w_{13}^{(1)} \\\\ w_{21}^{(1)} & w_{22}^{(1)} & w_{23}^{(1)} \\end{pmatrix}  + (b_{1}^{(1)}, b_{2}^{(1)}, b_{3}^{(1)}) $\n",
        "\\\n",
        "\\\n",
        "두번째 층\n",
        "\\\n",
        "$(a_{1}^{(2)}, a_{2}^{(2)}) = (z_{1}^{(1)}, z_{2}^{(1)}, z_{3}^{(1)}) \\begin{pmatrix} w_{11}^{(2)} & w_{12}^{(2)} \\\\ w_{21}^{(2)} & w_{22}^{(2)} \\\\ w_{31}^{(2)} & w_{32}^{(2)} \\end{pmatrix} + (b_{1}^{(2)}, b_{2}^{(2)}) $\n",
        "\\\n",
        "\\\n",
        "세번째 층\n",
        "\\\n",
        "$ (a_{1}^{(3)}, a_{2}^{(3)}) =  (z_{1}^{(2)}, z_{2}^{(2)})\\begin{pmatrix} w_{11}^{(2)} & w_{12}^{(2)} \\\\ w_{21}^{(2)} & w_{22}^{(2)} \\end{pmatrix}+ (b_{1}^{(3)}, b_{2}^{(3)}) $"
      ],
      "metadata": {
        "id": "3tTAXWJ6l5NH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_network(): # 초기 신경망 구현, 후에 학습과정이 추가되면 계속해서 업데이트 된다.\n",
        "    network = {} # 빈 딕셔너리\n",
        "    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]]) # 인공신경망 1층의 가중치 행렬, 위 식의 가중치 행렬을 표현 한 것.\n",
        "    network['b1'] = np.array([0.1, 0.2, 0.3]) # 인공신경망 1층의 bias 값 벡터, 위 식의 bias 벡터를 표현한 것.\n",
        "    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]]) # 두번째 층의 가중치 행렬\n",
        "    network['b2'] = np.array([0.1, 0.2]) # 두번째 층의 바이아스 벡터\n",
        "    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]]) # 세번째 층의 가중치 행렬\n",
        "    network['b3'] = np.array([0.1, 0.2]) # 세번째 층의 바이아스 벡터\n",
        "\n",
        "    return network\n",
        "\n",
        "init_network()"
      ],
      "metadata": {
        "id": "NRXoFl-2jShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(network, x): # 순전파 과정 구현\n",
        "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
        "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1 # 위 행렬식을 구현.\n",
        "    z1 = sigmoid(a1) # 가중합에 시그모이드 함수를 적용해 활성화 한다.\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    z2 = sigmoid(a2)\n",
        "    a3 = np.dot(z2, W3) + b3\n",
        "    y = identity_function(a3) # 항등함수 적용 한다. 후에 소프트 맥스 함수로 대체 한다.\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "J-JaMQtHpEbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "network = init_network()\n",
        "x = np.array([1.0, 0.5])\n",
        "y = forward(network, x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "BV9Lq91FwDlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax 함수 구현"
      ],
      "metadata": {
        "id": "BpHUZWi9EiiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    if x.dim == 2:   # x가 행렬일 경우 scalar = 0, vector = 1, matrix = 2, 위의 경우 vector를 처리하므로 후에 배치처리때 사용됨.\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis = 0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로를 방지하기위해 작성. 노트참고. , vector에 scalar를 빼면 numpy에서 알아서 원소별로 연산을 한다.\n",
        "    return np.exp(x) / np.sum(np.exp(x))  # normalize"
      ],
      "metadata": {
        "id": "7v1kTcQhwTO2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}