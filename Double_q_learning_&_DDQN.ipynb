{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMOd2wo39HEmKDdlmm3basS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Double_q_learning_%26_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c9jKVrJ2Qfc"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install imageio\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import imageio\n",
        "from gymnasium.wrappers import FrameStack, GrayScaleObservation\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wxV3LUHs2o9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수로 repeat을 추가.(게임 반복 횟수)\n",
        "\n",
        "def play_game(model, env, repeat, file_name, record : bool = False):\n",
        "\n",
        "    total_reward = 0\n",
        "    frames = []\n",
        "\n",
        "    for i in range(repeat):\n",
        "\n",
        "        obs, info = env.reset()\n",
        "        if record == True:\n",
        "            frames.append(env.render())\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(1)\n",
        "        if record == True:\n",
        "            frames.append(env.render())\n",
        "\n",
        "        while(terminated == False and truncated == False and info['lives' == 5]):\n",
        "\n",
        "            _, action = model.greedy_action(torch.tensor(np.array(obs)).to(DEVICE))\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            if record == True:\n",
        "                frames.append(env.render())\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "        if record == True:\n",
        "            with imageio.get_writer(f'/content/drive/MyDrive/동영상/{file_name}.mp4', fps=30, ) as video:\n",
        "                for frame in frames:\n",
        "                    video.append_data(frame)\n",
        "\n",
        "        total_reward = total_reward / repeat\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "# 모델 업데이트.\n",
        "\n",
        "def model_update(q_model, target_model, optimizer, buffer, batch_size):\n",
        "\n",
        "    obs_batch, action_batch, reward_batch, nobs_batch = buffer.sample(batch_size)\n",
        "\n",
        "    obs_batch = torch.tensor(np.array(obs_batch)).float().to(DEVICE)\n",
        "    action_batch = torch.tensor(np.array(action_batch)).float().to(DEVICE)\n",
        "    reward_batch = torch.tensor(np.array(reward_batch)).float().to(DEVICE)\n",
        "    nobs_batch = torch.tensor(np.array(nobs_batch)).float().to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, idx = q_model.greedy_action(nobs_batch) # 행동선택은 q모델의 정책에 따른다.\n",
        "        y = target_model.generate_q(nobs_batch, idx) # 선택된 행동에 대한 q값 평가는 타겟모델로 진행한다.\n",
        "        y = 0.99*y\n",
        "        y = y + reward_batch\n",
        "\n",
        "    q = q_model.generate_q(obs_batch, action_batch)\n",
        "\n",
        "    loss = F.mse_loss(y, q)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.item() # .item()은 텐서가 단일값을 포함하고 있을 경우 파이썬의 수자로 변환.\n",
        "\n",
        "    return loss\n",
        "\n",
        "def step_and_stack(model, env, buffer, epsilon, step_size):\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env.step(1)\n",
        "\n",
        "    for i in range(step_size):\n",
        "\n",
        "        if terminated == False and truncated == False and info['lives'] == 5:\n",
        "\n",
        "            action = model.epsilon_greedy_action(torch.tensor(np.array(obs)).to(DEVICE), epsilon)\n",
        "            nobs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            buffer.add([obs, action, reward, nobs])\n",
        "            obs = nobs\n",
        "\n",
        "        else:\n",
        "\n",
        "            obs, info = env.reset()\n",
        "            nobs, reward, terminated, truncated, info = env.step(1)\n",
        "            buffer.add([obs, 1, reward, nobs])\n",
        "            obs = nobs\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Qj4uxfoZ38bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def add(self, experience):\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "\n",
        "        else:\n",
        "            self.memory.pop(0) # 리스트가 클 경우 매우 비효율적. 대안으로 deque를 사용할 수 있지만 인데싱이 불편하고 느림.\n",
        "            self.memory.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "\n",
        "        obs_batch = [obs for obs, _, _, _ in sample]\n",
        "        action_batch = [action for _, action, _, _ in sample]\n",
        "        reward_batch = [reward for _, _, reward, _ in sample]\n",
        "        nobs_batch = [nobs for _, _, _, nobs in sample]\n",
        "\n",
        "        return obs_batch, action_batch, reward_batch, nobs_batch\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "PtcLvr7S6-Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQN(nn.module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(4, 16, kernel_size = 8, stride = 4),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Conv2d(16, 32, kernel_size = 4, stride =2),\n",
        "                                  nn.ReLU())"
      ],
      "metadata": {
        "id": "-ns9V9yqIzDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}