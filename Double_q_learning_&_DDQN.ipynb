{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMy3nORWTkMd4shxkBFvXbt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Double_q_learning_%26_DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6c9jKVrJ2Qfc"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install imageio\n",
        "!pip install imageio-ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import imageio\n",
        "from gymnasium.wrappers import FrameStack, GrayScaleObservation\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wxV3LUHs2o9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수로 repeat을 추가.(게임 반복 횟수)\n",
        "\n",
        "def play_game(model, env, repeat, file_name, record : bool = False):\n",
        "\n",
        "    total_reward = 0\n",
        "    frames = []\n",
        "\n",
        "    for i in range(repeat):\n",
        "\n",
        "        obs, info = env.reset()\n",
        "        if record == True:\n",
        "            frames.append(env.render())\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(1)\n",
        "        if record == True:\n",
        "            frames.append(env.render())\n",
        "\n",
        "        while(terminated == False and truncated == False and info['lives'] == 5):\n",
        "\n",
        "            _, action = model.greedy_action(torch.tensor(np.array(obs)).to(DEVICE))\n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            if record == True:\n",
        "                frames.append(env.render())\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "    if record == True:\n",
        "        with imageio.get_writer(f'/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/동영상/{file_name}.mp4', fps=30, ) as video:\n",
        "            for frame in frames:\n",
        "                video.append_data(frame)\n",
        "\n",
        "    total_reward = total_reward / repeat\n",
        "\n",
        "    return total_reward\n",
        "\n",
        "# 모델 업데이트.\n",
        "\n",
        "def model_update(q_model, target_model, optimizer, buffer, batch_size):\n",
        "\n",
        "    obs_batch, action_batch, reward_batch, nobs_batch = buffer.sample(batch_size)\n",
        "\n",
        "    obs_batch = torch.tensor(np.array(obs_batch)).float().to(DEVICE)\n",
        "    action_batch = torch.tensor(np.array(action_batch)).float().to(DEVICE)\n",
        "    reward_batch = torch.tensor(np.array(reward_batch)).float().to(DEVICE)\n",
        "    nobs_batch = torch.tensor(np.array(nobs_batch)).float().to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        _, idx = q_model.greedy_action(nobs_batch) # 행동선택은 q모델의 정책에 따른다.\n",
        "        y = target_model.generate_q(nobs_batch, idx) # 선택된 행동에 대한 q값 평가는 타겟모델로 진행한다.\n",
        "        y = 0.99*y\n",
        "        y = y + reward_batch\n",
        "\n",
        "    q = q_model.generate_q(obs_batch, action_batch)\n",
        "\n",
        "    loss = F.mse_loss(y, q)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss = loss.item() # .item()은 텐서가 단일값을 포함하고 있을 경우 파이썬의 수자로 변환.\n",
        "\n",
        "    return loss\n",
        "\n",
        "def step_and_stack(model, env, buffer, epsilon, step_size):\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env.step(1)\n",
        "\n",
        "    for i in range(step_size):\n",
        "\n",
        "        if terminated == False and truncated == False and info['lives'] == 5:\n",
        "\n",
        "            action = model.epsilon_greedy_action(torch.tensor(np.array(obs)).to(DEVICE), epsilon)\n",
        "            nobs, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "            buffer.add([obs, action, reward, nobs])\n",
        "            obs = nobs\n",
        "\n",
        "        else:\n",
        "\n",
        "            obs, info = env.reset()\n",
        "            nobs, reward, terminated, truncated, info = env.step(1)\n",
        "            buffer.add([obs, 1, reward, nobs])\n",
        "            obs = nobs\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Qj4uxfoZ38bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ExperienceReplayMemory:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "\n",
        "    def add(self, experience):\n",
        "\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(experience)\n",
        "\n",
        "        else:\n",
        "            self.memory.pop(0) # 리스트가 클 경우 매우 비효율적. 대안으로 deque를 사용할 수 있지만 인데싱이 불편하고 느림.\n",
        "            self.memory.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        sample = random.sample(self.memory, batch_size)\n",
        "\n",
        "        obs_batch = [obs for obs, _, _, _ in sample]\n",
        "        action_batch = [action for _, action, _, _ in sample]\n",
        "        reward_batch = [reward for _, _, reward, _ in sample]\n",
        "        nobs_batch = [nobs for _, _, _, nobs in sample]\n",
        "\n",
        "        return obs_batch, action_batch, reward_batch, nobs_batch\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "PtcLvr7S6-Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDQN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(nn.Conv2d(4, 32, kernel_size = 8, stride = 4),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
        "                                  nn.ReLU(),\n",
        "                                  nn.Conv2d(64, 64, kernel_size = 3, stride = 1),\n",
        "                                  nn.ReLU())\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(3136, 512),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(512, 4))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.preprocessing(x)\n",
        "\n",
        "        x = self.conv(x)\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def greedy_action(self, x):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.forward(x)\n",
        "            x, idx = torch.max(x, 1)\n",
        "\n",
        "        return x, idx\n",
        "\n",
        "    def epsilon_greedy_action(self, x ,epsilon):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            if random.random() > epsilon:\n",
        "                x, idx = self.greedy_action(x)\n",
        "                x = idx.item()\n",
        "\n",
        "            else:\n",
        "                x = random.randrange(4)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def generate_q(self, x, action_batch):\n",
        "\n",
        "        x = self.forward(x)\n",
        "        action_batch = action_batch.unsqueeze(1).long()\n",
        "        q = torch.gather(x, 1, action_batch)\n",
        "        q = q.squeeze(1)\n",
        "\n",
        "        return q\n",
        "\n",
        "    def preprocessing(self, x):\n",
        "\n",
        "        # x = np.array(x)\n",
        "        # x = torch.tensor(x)\n",
        "\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        x = x[:, :, 34:-16, :]\n",
        "        resize_transform = transforms.Resize((84, 84))\n",
        "        x = resize_transform(x)\n",
        "        x = x.float() / 255.\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "-ns9V9yqIzDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_model = DDQN().to(DEVICE)\n",
        "t_model = DDQN().to(DEVICE)\n",
        "t_model.load_state_dict(q_model.state_dict()) # Q모델의 파라미터를 타겟모델에 복사한다."
      ],
      "metadata": {
        "id": "d9WqAp5TNCHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#환경 초기화\n",
        "env = gym.make(\"ALE/Breakout-v5\", render_mode='rgb_array', obs_type='grayscale')\n",
        "env = FrameStack(env, 4)\n",
        "obs, info = env.reset()"
      ],
      "metadata": {
        "id": "xgX78WfLNElD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 매개변수 초기화.\n",
        "epsilon = 1\n",
        "buffer = ExperienceReplayMemory(1000000)\n",
        "optimizer = optim.RMSprop(q_model.parameters(), lr=0.00025, alpha=0.95, eps=1e-6, momentum = 0.95)\n",
        "reward_r = []\n",
        "loss_r = []\n",
        "epsilon_r = []"
      ],
      "metadata": {
        "id": "bZUUBPYhPT0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step_and_stack(q_model, env, buffer, epsilon, step_size = 50000)\n",
        "\n",
        "for i in tqdm(range(1000000)):\n",
        "\n",
        "    step_and_stack(q_model, env, buffer, epsilon, step_size = 4)\n",
        "\n",
        "    loss = model_update(q_model, t_model, optimizer, buffer, 32)\n",
        "    loss_r.append(loss)\n",
        "\n",
        "    if i % 10000 == 0:\n",
        "\n",
        "        t_model.load_state_dict(q_model.state_dict())\n",
        "        torch.save(q_model, f'/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/model/{str(i)}.pth')\n",
        "\n",
        "        r = play_game(q_model, env, 5,  str(i), record = True)\n",
        "\n",
        "        print(f\"total reward : {r}\")\n",
        "        reward_r.append(r)\n",
        "\n",
        "        # 훈련 과정 백업.\n",
        "\n",
        "        # reward_r 리스트를 reward.txt 파일에 저장\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/reward.txt', 'w', encoding='utf-8') as file:\n",
        "            for item in reward_r:\n",
        "                file.write(\"%s\\n\" % item)\n",
        "\n",
        "        # loss_r 리스트를 loss.txt 파일에 저장\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/loss.txt', 'w', encoding='utf-8') as file:\n",
        "            for item in loss_r:\n",
        "                file.write(\"%s\\n\" % item)\n",
        "\n",
        "        # epsilon_r 리스트를 epsilon.txt 파일에 저장\n",
        "        with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/epsilon.txt', 'w', encoding='utf-8') as file:\n",
        "            for item in epsilon_r:\n",
        "                file.write(\"%s\\n\" % item)\n",
        "\n",
        "    if i % 500 == 0:\n",
        "        epsilon = max(epsilon - 0.00045, 0.1)\n",
        "        epsilon_r.append(epsilon)"
      ],
      "metadata": {
        "id": "Eazc0HkNPWIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # reward_r 리스트로 데이터를 불러오기\n",
        "# reward_r = []\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/reward.txt', 'r', encoding='utf-8') as file:\n",
        "#     for line in file:\n",
        "#         reward_r.append(float(line.strip()))\n",
        "\n",
        "# # loss_r 리스트로 데이터를 불러오기\n",
        "# loss_r = []\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/loss.txt', 'r', encoding='utf-8') as file:\n",
        "#     for line in file:\n",
        "#         loss_r.append(float(line.strip()))\n",
        "\n",
        "# # epsilon_r 리스트로 데이터를 불러오기\n",
        "# epsilon_r = []\n",
        "# with open('/content/drive/MyDrive/Colab Notebooks/딥러닝/포트폴리오/DDQN/log/epsilon.txt', 'r', encoding='utf-8') as file:\n",
        "#     for line in file:\n",
        "#         epsilon_r.append(float(line.strip()))"
      ],
      "metadata": {
        "id": "jG3GCbpdTLpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.plot(reward_r, label = 'Reward', color = 'red')\n",
        "# plt.xlabel('Step(5000)')\n",
        "# plt.ylabel('Reward')\n",
        "# plt.title('10p Reward')\n",
        "# plt.legend()\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "fSWp9D_LTPsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "강화학습에서 하이퍼파라미터 탐색에 사용될 수 있는 여러 방법들이 있으며, 이들은 계산 비용, 탐색 공간의 크기, 그리고 필요한 정밀도에 따라 선택됩니다. 그리드 서치(Grid Search)가 전통적이고 간단한 방법이지만 높은 계산 비용 때문에 실제 큰 문제에는 적합하지 않을 수 있습니다. 따라서 다음과 같은 대안적인 방법들이 고려될 수 있습니다:\n",
        "\n",
        "랜덤 서치(Random Search): 랜덤 서치는 하이퍼파라미터의 값들을 무작위로 선택하여 탐색 공간 내에서 여러 구성을 시도합니다. 이 방법은 간단하며 때로는 예상치 못한 좋은 결과를 빠르게 찾아낼 수 있습니다. 계산 비용이 높은 그리드 서치에 비해 효율적일 수 있습니다.\n",
        "\n",
        "베이지안 최적화(Bayesian Optimization): 이 방법은 성능 함수의 과거 평가를 기반으로 하여 하이퍼파라미터의 가장 유망한 값들을 예측합니다. 이 방법은 비교적 적은 수의 평가로 좋은 성능을 찾을 수 있도록 설계되었으며, 복잡한 하이퍼파라미터 공간에서 효과적일 수 있습니다.\n",
        "\n",
        "진화 알고리즘(Evolutionary Algorithms): 이들은 자연 선택과 유전학의 원리를 모방하여 최적의 하이퍼파라미터 구성을 \"진화\"시키는 방법입니다. 초기에 임의로 선택된 하이퍼파라미터 세트는 성능에 따라 선택되고 교차 및 변이를 통해 새로운 세트를 생성합니다. 이 과정은 특정 조건이 충족될 때까지 반복됩니다.\n",
        "\n",
        "코사인 어닐링(Cosine Annealing): 이 방법은 초기에 높은 탐색 범위에서 시작하여 시간이 지남에 따라 점차 탐색 범위를 줄여나가는 방식으로 작동합니다. 이는 물리학에서 어닐링 과정을 모방한 것으로, 최적화 과정에서 전역 최소값에 접근할 수 있게 합니다.\n",
        "\n",
        "하이퍼밴드(Hyperband): 하이퍼밴드는 리소스 할당의 효율성을 최대화하기 위해 설계된 방법입니다. 다양한 구성에 대한 초기 평가를 수행한 후, 가장 잘 수행되는 구성에 더 많은 리소스를 집중적으로 할당합니다.\n",
        "\n",
        "강화학습 프로젝트에서는 이러한 방법들 중 하나 또는 여러 개를 조합하여 하이퍼파라미터 탐색을 수행할 수 있습니다. 선택된 방법은 특정 문제의 특성, 사용 가능한 계산 자원, 그리고 원하는 실험의 속도와 정확도에 따라 달라질 수 있습니다."
      ],
      "metadata": {
        "id": "-lOM6DUeUp32"
      }
    }
  ]
}