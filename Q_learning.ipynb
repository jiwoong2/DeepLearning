{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PpeiGU_g4YnL"
      ],
      "authorship_tag": "ABX9TyO1hzwmp4z663Fr+XXSNVxW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "2aRi4vPqe5QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FrozenLake-v1(기본적인 격자 환경)"
      ],
      "metadata": {
        "id": "PpeiGU_g4YnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 생성\n",
        "env = gym.make('FrozenLake-v1')\n",
        "\n",
        "# 관측 공간 (Observation Space) 살펴보기\n",
        "print(\"Observation Space:\")\n",
        "print(\"Type:\", env.observation_space) # 관측공간 특성(이산적)\n",
        "print(\"Number of States:\", env.observation_space.n)  # 16개의 타일선택 가능\n",
        "\n",
        "# 행동 공간 (Action Space) 살펴보기\n",
        "print(\"\\nAction Space:\")\n",
        "print(\"Type:\", env.action_space)\n",
        "print(\"Number of Actions:\", env.action_space.n)\n",
        "\n",
        "# 보상의 범위 살펴보기\n",
        "print(\"\\nReward Range:\")\n",
        "print(\"Min Reward:\", env.reward_range[0])\n",
        "print(\"Max Reward:\", env.reward_range[1])\n",
        "\n",
        "# 맵 출력\n",
        "env.unwrapped.desc"
      ],
      "metadata": {
        "id": "H0pF6eDq4V96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hghp-I58e1WI"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "\n",
        "    def __init__(self, env, learning_rate=0.1, discount_rate=0.95, exploration_rate=1.0, exploration_decay=0.99):\n",
        "\n",
        "        self.env = env # 환경객체\n",
        "        self.learning_rate = learning_rate # 학습률\n",
        "        self.discount_rate = discount_rate # 할인률\n",
        "        self.exploration_rate = exploration_rate # 탐험률\n",
        "        self.exploration_decay = exploration_decay # 탐험률 감소율\n",
        "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n)) # Q테이블\n",
        "\n",
        "    def train(self, episodes=1000):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "\n",
        "            #  ε-greedy 정책을 구현\n",
        "            while not done:\n",
        "                if np.random.rand() < self.exploration_rate:\n",
        "                    action = self.env.action_space.sample()  # Explore. action space에서 무작위 선택\n",
        "                else:\n",
        "                    action = np.argmax(self.q_table[state])  # Exploit.  현재 상태(state)에 대한 Q 테이블에서 가장 높은 값의 인덱스를 반환.\n",
        "\n",
        "                next_state, reward, done, _ = self.env.step(action) # 행동 후 다음상태, 보상, 에피소드 종료여부를 반환.\n",
        "                old_value = self.q_table[state, action] # 상태에 대한 행동값의 Q값저장.\n",
        "                next_max = np.max(self.q_table[next_state]) # 다음상태에서의 가장큰 Q테이블 값 저장.\n",
        "\n",
        "                # Q-learning formula\n",
        "                new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_rate * next_max) # Q값 업데이트.\n",
        "                self.q_table[state, action] = new_value\n",
        "\n",
        "                state = next_state\n",
        "\n",
        "            # Decay exploration rate\n",
        "            self.exploration_rate *= self.exploration_decay # 탐험률 감쇠.\n",
        "\n",
        "    def test(self, episodes=100):\n",
        "\n",
        "        total_rewards = 0\n",
        "\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.argmax(self.q_table[state])  # Exploit learned values\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                total_rewards += reward\n",
        "\n",
        "        avg_reward = total_rewards / episodes\n",
        "\n",
        "        print(f\"Average Reward: {avg_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "env = gym.make('FrozenLake-v1')\n",
        "agent = QLearningAgent(env = env, learning_rate=0.1, discount_rate=0.95, exploration_rate=1.0, exploration_decay=0.999)\n",
        "agent.train(episodes=1000)\n",
        "agent.test(episodes=100)"
      ],
      "metadata": {
        "id": "u9ll7RCXe8RN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CartPole-v1(막대기 세우기)"
      ],
      "metadata": {
        "id": "-Aq2pi6n4cUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 매시간다계마다 reward 1을 얻음\n",
        "\n",
        "# 환경 생성\n",
        "env = gym.make('CartPole-v1', new_step_api=True)\n",
        "\n",
        "# 관측 공간 (Observation Space) 살펴보기\n",
        "print(\"Observation Space:\")\n",
        "print(\"Shape:\", env.observation_space.shape) # 관측공간의 차원\n",
        "\n",
        "# 0: 카트 위치, 1: 카트 속도, 2: 막대기 각도, 3: 막대기 각속도\n",
        "print(\"High:\", env.observation_space.high) # 각 차원별 관측치의 상한\n",
        "print(\"Low:\", env.observation_space.low) # 각 차원별 관측치의 하한\n",
        "\n",
        "# 행동 공간 (Action Space) 살펴보기\n",
        "print(\"\\nAction Space:\")\n",
        "print(\"Number of Actions:\", env.action_space.n) # 에이전트가 취할 수 있는 행동 수\n",
        "\n",
        "# 보상의 범위 살펴보기\n",
        "print(\"\\nReward Range:\")\n",
        "print(\"Min Reward:\", env.reward_range[0]) # 최소 보상 값\n",
        "print(\"Max Reward:\", env.reward_range[1]) # 최대 보상 값"
      ],
      "metadata": {
        "id": "9b054Qgo4vy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "metadata": {
        "id": "E_BGn7fO7IfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscretizedQLearningAgent:\n",
        "    def __init__(self, n_bins=10, n_episodes=1000, min_lr=0.1, min_epsilon=0.1, discount=0.99):\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.n_bins = n_bins  # 이산화할 구간의 수\n",
        "        self.n_episodes = n_episodes  # 총 에피소드 수\n",
        "        self.min_lr = min_lr  # 최소 학습률\n",
        "        self.min_epsilon = min_epsilon  # 최소 탐색률\n",
        "        self.discount = discount  # 할인율\n",
        "\n",
        "        # 상태 공간을 이산화하기 위한 구간 계산\n",
        "        self.bins = [\n",
        "            np.linspace(-4.8, 4.8, self.n_bins),  # 카트 위치\n",
        "            np.linspace(-4, 4, self.n_bins),  # 카트 속도\n",
        "            np.linspace(-0.418, 0.418, self.n_bins),  # 막대기 각도\n",
        "            np.linspace(-4, 4, self.n_bins)  # 막대기 각속도\n",
        "        ]\n",
        "\n",
        "        self.q_table = np.zeros([n_bins] * len(self.env.observation_space.high) + [self.env.action_space.n])\n",
        "\n",
        "    def discretize(self, observation):\n",
        "        # 연속적인 관측값을 이산화된 상태로 변환\n",
        "        binned = []\n",
        "        for i in range(len(observation)):\n",
        "            binned.append(np.digitize(observation[i], self.bins[i]) - 1) #  주어진 값이 어느 구간에 속하는지를 계산하여 반환. digitize가 1base 인덱스를반환하므로 -1을 붙임.\n",
        "        return tuple(binned)\n",
        "\n",
        "    def train(self):\n",
        "        for e in range(self.n_episodes):\n",
        "            current_state = self.discretize(self.env.reset()) # 이산화.\n",
        "\n",
        "            # 로그함수를 사용한 감쇠. 로그 함수의 특성상 초기에는 값의 변화가 크고, 에피소드가 진행될수록 변화가 완만해지는 특성을 이용.\n",
        "            lr = max(self.min_lr, min(1.0, 1.0 - np.log10((e + 1) / 25))) # 학습률 감쇠.\n",
        "            epsilon = max(self.min_epsilon, min(1, 1.0 - np.log10((e + 1) / 25))) #탐헙률감쇠\n",
        "\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                if np.random.random() < epsilon:\n",
        "                    action = self.env.action_space.sample()  # 탐색\n",
        "                else:\n",
        "                    action = np.argmax(self.q_table[current_state])  # 이용\n",
        "\n",
        "                obs, reward, done, _ = self.env.step(action)\n",
        "                new_state = self.discretize(obs)\n",
        "\n",
        "                # Q-table 업데이트\n",
        "                self.q_table[current_state + (action,)] += lr * (reward + self.discount * np.max(self.q_table[new_state]) - self.q_table[current_state + (action,)])\n",
        "                current_state = new_state\n",
        "\n",
        "    def test(self):\n",
        "        total_reward = 0\n",
        "        for _ in range(100):\n",
        "            state = self.discretize(self.env.reset())\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = np.argmax(self.q_table[state])\n",
        "                state, reward, done, _ = self.env.step(action)\n",
        "                state = self.discretize(state)\n",
        "                total_reward += reward\n",
        "        print(f\"Average reward: {total_reward / 100}\")"
      ],
      "metadata": {
        "id": "Dsbo7zmt4nhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DiscretizedQLearningAgent()\n",
        "agent.train()\n",
        "agent.test()"
      ],
      "metadata": {
        "id": "lFyKZNUp4yDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQL"
      ],
      "metadata": {
        "id": "OhpSMm67Bsaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 정의\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "PSyUsQOjRfm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQL 에이전트\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = QNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        q_values = self.model(state)\n",
        "        return np.argmax(q_values.detach().numpy())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size) # 무작위샘플링\n",
        "\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "            reward = torch.FloatTensor([reward])\n",
        "            done = torch.FloatTensor([done])\n",
        "\n",
        "            q_values = self.model(state)\n",
        "            next_q_values = self.model(next_state)\n",
        "            q_value = q_values[0][action]\n",
        "            next_q_value = reward + self.gamma * next_q_values.max(1)[0] * (1 - done)\n",
        "\n",
        "            loss = (q_value - next_q_value.detach()).pow(2).mean()\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon_decay*self.epsilon)  # Epsilon 감소"
      ],
      "metadata": {
        "id": "f9Npdy1ORgyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 환경 초기화 및 에이전트 생성\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# 학습\n",
        "episodes = 1000\n",
        "batch_size = 32\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    agent.replay(batch_size)\n",
        "    print(f\"Episode: {e+1}, Total reward: {total_reward}, Epsilon: {agent.epsilon}\")"
      ],
      "metadata": {
        "id": "_TdpQPqbBt0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 신경망 모델 정의\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# DQL 에이전트\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = QNetwork(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            q_values = self.model(state)\n",
        "        return q_values.max(1)[1].item()\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        next_q_values = self.model(next_states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_value = next_q_values.max(1)[0]\n",
        "        expected_q_value = rewards + self.gamma * next_q_value * (1 - dones)\n",
        "\n",
        "        loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon_decay*self.epsilon)  # Epsilon 감소\n",
        "\n",
        "# 환경 및 에이전트 초기화\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "agent = DQNAgent(state_dim, action_dim)\n",
        "\n",
        "# 학습\n",
        "episodes = 1000\n",
        "batch_size = 64\n",
        "\n",
        "for e in range(episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    agent.replay(batch_size)\n",
        "    if e % 10 == 0:\n",
        "        print(f\"Episode: {e+1}, Total reward: {total_reward}, Epsilon: {agent.epsilon}\")\n"
      ],
      "metadata": {
        "id": "UZesdQW1YFKH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}