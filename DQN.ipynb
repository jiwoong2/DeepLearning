{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "17IQUFTQxyWOyntTEy0-yRcar6TKoqHW_",
      "authorship_tag": "ABX9TyO61ZgQs0fResgq9AU6LUj5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note\n",
        "\n",
        "강화학습에서는 생소한 개념이 많이 등장한다. 강화학습에서 많이 사용되는 수식들이 내포한 의미를 깊이 이해하기위해 직관적으로 이해가능한 수식으로부터 다양한 알고리즘의 업데이트 공식을 유도해본다."
      ],
      "metadata": {
        "id": "kv9BvLreqNIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.일단 직관적으로 이해가능한 MDP의 3개의 수식으로부터 시작한다.\n",
        "\n",
        "i) Return G는 현 상태에서 에피소드가 끝날때까지 진행됐을때 얻는 reward의 합으로 discounter factor = $ \\gamma $을 적용해 미래보상이 할인돼서 계산된다.\n",
        "\n",
        "$G_{t}  \\triangleq R_{t}+\\gamma R_{t+1}+\\gamma^{2} R_{t+2}+\\gamma^{3} R_{t+3}+ ...$\n",
        "\n",
        "ii) state value function $ V(S_{t}) $는 상태가치를 나타내는 함수로 현 상태로부터 에피소드가 끝날때까지 진행됬을때 얻을 수 있는 Returnd의 기댓값이다.\n",
        "\n",
        "$ V(s_{t})  \\triangleq \\int_{a_{t}:a_{\\infty}}G_{t}p(a_t,s_{t+1},a_{t+1}, ...|s_{t})d_{a_{t}:a_{\\infty}} $\n",
        "\n",
        "iii) action value function $Q(s_{t},a_{t})$는 행동가치를 나타내는 함수로 현상태에서 가능한 어떤 행동에서부터 에피소드가 끝날때까지 진행됬을때 얻을 수 있는 Return의 기댓값이다.\n",
        "\n",
        "$ Q(s_t,a_t) \\triangleq \\int_{s_{t+1}:a_{\\infty}}G_{t}p(s_{t+1},a_{t+1}, ...|s_{t})d_{s_{t+1}:a_{\\infty}} $\n",
        "\n",
        "상태가치함수와 행동가치함수 모두 다차원 확률변수에 대한 조건부기댓값으로 표현되므로 수식으로부터 정의된 개념을 직관적으로 이해할 수 있다.\n",
        "\n",
        "2.위의 수식을 변형해 Bellman equation을 유도하는 과정에서 새로운 인사이트를 얻는다.\n",
        "\n",
        "i) 상태가치함수 $V(s_t)$를 행동가치함수$Q(s_{t},a_{t})$로 표현하기.\n",
        "\n",
        "$p(a_t,s_{t+1},a_{t+1}, ...|s_{t}) = p(s_{t+1},a_{t+1}, ...|s_{t},a_t)p(a_t|s_t)\n",
        "$로부터 $V(s_t)$를 변형된 식으로 표현하면,\n",
        "\n",
        "$ V(s_{t})  \\triangleq \\int_{a_t} \\int_{s_{t+1}:a_{\\infty}}G_{t}p(s_{t+1},a_{t+1}, ...|s_{t},a_t)d_{s_{t+1}:a_{\\infty}} p(a_t|s_t)da_t $로 표혀되고 이식을 다시 행동가치함수$Q(s_{t},a_{t})$로 표현하면\n",
        "\n",
        "$ \\int_{a_t}Q(s_t,a_t)p(a_t|s_t)da_t $로 표현된다.\n",
        "\n",
        "위 식으로부터 얻을 수 있는 인사이트는 상태가치함수 $V(s_t)$가 행동가치함수$Q(s_{t},a_{t})$의 기댓값으로 표현된다는 점 이다. 그러니까 상태에대한 가치는 그 상태에서 policy에 따라 취할 수 있는 행동가치의 평균, 즉 기댓값인 것 이다.\n",
        "\n",
        "optimal policy($p^*(a_t,s_t) = \\delta(a_t-a_t^*)$)\n",
        "여기서 $a_t^* = argmax_{a_t}Q^*(s_t,a_t)$\n",
        "\n",
        "또, 상태가치함수 $V(s_t)$를 maximzie하는 policy(행동정책) ($p(a_t|s_t)$) 즉 optimal policy는 행동가치함수 $Q(s_{t},a_{t})$가 가장 큰값만을 갖게하는 정책(greedy action)임을 알 수 있다.(단, 행동가치함수$Q(s_{t},a_{t})$가 $Q^*(s_{t},a_{t})$인 경우만, 즉 미래시점 행동이 모두 optimal policy에 의해 선택된경우)\n",
        "\n",
        "ii) 상태가치함수 $V(s_t)$를 다음 시점의 상태가치함수 $V(s_t+1)$로 표현하기.\n",
        "\n",
        "$ p(a_t,s_{t+1},a_{t+1}, ...|s_{t}) = p(a_{t+1}, ...|s_t,a_t, s_{t+1})p(a_t,s_{t+1}|s_t) $ 에서 다시 MDP의 특성으로 부터\n",
        "\n",
        "$ p(a_{t+1}, ...|s_t,a_t, s_{t+1})p(a_t,s_{t+1}|s_t) = p(a_{t+1}, ...|s_{t+1})p(a_t,s_{t+1}|s_t) $으로 표현되고 또,\n",
        "\n",
        "$G_{t}  \\triangleq R_{t}+\\gamma R_{t+1}+\\gamma^{2} R_{t+2}+\\gamma^{3} R_{t+3}+ ... = R_t+\\gamma G_{t+1} $\n",
        "\n",
        "위의 두 변현식으로 상태가치함수 $V(s_t)$를 다시 표현하면\n",
        "\n",
        "$ \\int_{a_t,s_{t+1}} \\int_{a_{t+1}:a_{\\infty}} (R_t+\\gamma G_{t+1})p(a_{t+1}, ...|s_{t+1})d_{a_{t+1}:a_{\\infty}} p(a_t,s_{t+1}|s_t)d_{a_t,s_{t+1}} $로 표현되고 이를 다시 다음시점의 상태가치함수 $V(s_t+1)$로 표현하면\n",
        "\n",
        "$ \\int_{a_t,s_{t+1}} (R_t+\\gamma V(s_{t+1}))p(a_t,s_{t+1}|s_t)d_{a_t,s_{t+1}} $로 표현될 수 있다. 여기서 다시\n",
        "\n",
        "$ p(a_t,s_{t+1}|s_t) = p(s_{t+1}|s_t,a_t)p(a_t|s_t) $로 변형해 표혀하면\n",
        "\n",
        "$ \\int_{a_t,s_{t+1}} (R_t+\\gamma V(s_{t+1}))p(s_{t+1}|s_t,a_t)p(a_t|s_t)d_{a_t,s_{t+1}} $로 표현된다.\n",
        "\n",
        "위의 수식이 Bellman equation으로 이로부터 얻을 수 있는 인사이트는 상태가치함수 $V(s_t)$가 다음시점의 상태가치함수 $V(s_{t+1})$, transition probablity($p(s_{t+1}|s_t,a_t)$), policy($p(a_t|s_t)$)로 표현된다는 점 이다. 특히 policy는 행동정책으로 상태가치함수를 maximize하는 policy, 즉 optimal policy를 찾는것은 강화학습의 주 목적이다.\n",
        "\n",
        "비슷한 과정으로 행동가치함수$Q(s_{t},a_{t})$또한 다음 상태의 상태가치함수로 표현되거나 다음상태의 행동가치함수로 표현될 수 있다.\n",
        "\n",
        "$ Q(a_t,s_t) = \\int_{s_{t+1},a_{t+1}}(R_t + \\gamma Q(s_{t+1},a_{t+1}))p(s_{t+1}|s_t,a_t)p(a_{t+1}|s_{t+1})d_{s_{t+1},a_{t+1}} $\n",
        "\n",
        "이 또한 Bellman equation으로 상태가치함수의 Bellman equation과의 차이점은 다음상태의 행동정책의 영향을 받는다는 것 이다.\n",
        "\n",
        "3.$Q^*$는 어떻게 구해지는가?\n",
        "\n",
        "i) Monte_Carlo\n",
        "\n",
        "먼저 큰 수의 법칙\n",
        "\n",
        "$ E[X]=\\int_x xp(x)dx \\approx \\frac{1}{N}\\sum^N_{i=1} x_i $ 에 따라\n",
        "\n",
        "$ Q(s_t,a_t) \\triangleq \\int_{s_{t+1}:a_{\\infty}}G_{t}p(s_{t+1}:a_{\\infty}|s_{t})d_{s_{t+1}:a_{\\infty}} \\approx \\frac{1}{N}\\sum G^i_t  $ 으로 근사하 수 있다.\n",
        "\n",
        "위 수식이 이것이 Monte_carlo method이며 이로부터 얻을 수 있느 인사이트는 어떤상태에서 에피소드를 충분히 많이 진행하면서 얻어진 Q값의 평균으로 Q값을 $ Q^* $로 근사할 수 있다는 것 이다. 한계로는 수식을 보면 알 수 있듯이 에피소드를 끝까지 진행해야만 Q값이 업데이트 된다는 점 이다.\n",
        "\n",
        "II) Temporal difference\n",
        "\n",
        "먼저 위에서 유도한 행동가치함수의 Bellman equation\n",
        "\n",
        "$ Q(a_t,s_t) = \\int_{s_{t+1},a_{t+1}}(R_t + \\gamma Q(s_{t+1},a_{t+1}))p(s_{t+1}|s_t,a_t)p(a_{t+1}|s_{t+1})d_{s_{t+1},a_{t+1}} $ 에 큰수의 법칙을 적용해\n",
        "\n",
        "$ \\approx \\frac{1}{N}\\sum^N_{i=1}(R^{(I)}_t + \\gamma Q(s^{(i)}_{t+1},a^{(i)}_{t+1})) \\triangleq \\bar{Q_N} $로 근사할 수 있다.\n",
        "여기서 R이 sample인 이유는 transition probablity때문으로 어떤 행동에 대한 결과 또한 확률적이기 때문이다.\n",
        "\n",
        "Temporal difference가 Monte_carlo와 다른점은 Monte_carlo는 에피소드가 끝날때까지 진행해야만 Q값을 엄데이트할 수 있는 반면 Temperal difference는 위 수식에서 보는바와 같이 바로 다음시점의 Q값만으로 현시점의 Q값을 바로 업데이트 가능하다는 점 이다.\n",
        "\n",
        "! $R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1}) - \\bar{Q}_{N-1}$는 따로 TD error라고 부르고 TD error에서 다음시점의 보상과 Q의 샘플 $ R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1}) $을 TD-taget이라고 한다.\n",
        "\n",
        "위 식을 변형해 다시 적으면\n",
        "\n",
        "$ = \\frac{1}{N}(\\bar{Q}_{N-1}(N-1) + R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1})) $로 나타낼수 있고 이를 전개하면\n",
        "\n",
        "$ = \\bar{Q}_{N-1} + \\frac{1}{N}(R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1}) - \\bar{Q}_{N-1}) $ 로 표현할 수 있다.\n",
        "\n",
        "이를 1-Step TD 또는 Incremental monte_carlo update라고 한며 이로부터 얻을 수 있는 인사이트는 큰수의 법칙으로부터 $Q^*$를 근사할때 이전 샘플들의 평균에서 다음 샘플을 합한 평균을 구할때 처음부터 계산하지 않고 약간의 변형으로 이전 평균값과 다음 샘플값만으로 다음평균값을 구할 수 있다는 것 이다.\n",
        "\n",
        "또, 위식을 다시 변형해\n",
        "\n",
        "$ \\frac{1}{N} = \\alpha $ 로 부터\n",
        "\n",
        "$ (1-\\alpha)\\bar{Q}_{N-1} + \\alpha(R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1})) $로 표현가능하며 이는 SARSA의 업데이트 공식이다.\n",
        "\n",
        "MC와  SARSA를 비교해보면 SARSA는 업데이트 공식의 원본 $ Q(a_t,s_t) = \\int_{s_{t+1},a_{t+1}}(R_t + \\gamma Q(s_{t+1},a_{t+1}))p(s_{t+1}|s_t,a_t)p(a_{t+1}|s_{t+1})d_{s_{t+1},a_{t+1}} $으로부터 알 수 있듯이 다음상태의 Q값이 $ Q^*$가 아닌 이상 업데이트는 완전하지 않으며 이로부터 편향이 발생할 수 있다는 것 이다.(biased sample) 대신 에피소드가 끝날때가지 진행해야하는(unbiased sample) MC는 variance가 너무 커서 수렴이 쉽지 않다는 단점이 있다.\n",
        "\n",
        "4.Q러닝\n",
        "\n",
        "Q러닝의 업데이트 공식은 SARSA와 같다. 차이점은 Behaier policy와 Target policy가 다르다는 점이다(off policy). 즉 SARSA는 행동정책과 업데이트 샘플을 선택하는 타겟정책이 같고(on policy) Q러닝은 이것이 다르다는 것 이다.\n",
        "\n",
        "$ Q(a_t,s_t) = \\int_{s_{t+1},a_{t+1}}(R_t + \\gamma Q(s_{t+1},a_{t+1}))p(s_{t+1}|s_t,a_t)p(a_{t+1}|s_{t+1})d_{s_{t+1},a_{t+1}} $\n",
        "\n",
        "$ \\approx (1-\\alpha)\\bar{Q}_{N-1} + \\alpha(R^{N}_t + \\gamma Q(s^{N}_{t+1}, a^{N}_{t+1})) $\n",
        "\n",
        "위 수식에서 Target policy는 $ p(a_{t+1}|s_{t+1}) $ Behavior policy는 $ p(s_{t+1}|s_t,a_t) $이다.\n",
        "\n",
        "SARSA는 현상태에서 행동후 다음 상태에 또한 행동정책에 의해 샘플링된 Q값으로 현 Q값이 업데이트 돼고 Q러닝은 현상태서 행동후 다음 상태에서 현동정책과는 별개로 다른 전략으로 Q값을 선택해 업데이트할 수 있다. 이로부터 얻을 수 있는이점은 행동 정책의 탐험률은 높여서 optimal에 도달할 여러 route를 탐색하면서도 업데이트는 가장 높은 Q값으로 업데이트(greedy polcy)하는것이 가능하다는 것 이다.\n",
        "\n",
        "정리해 보자면 MDP위에서 이뤄지는 task를 해결하기위해 상태가치함수나 행동가치함수를 최대화하는 전략을 사용할 수 있고 그 방법으로 Monte-carlo method와 Temporal difference이 있다. 그 중 Temporal difference는 on-policy와 off-policy로 나눌 수 있으며 각각 SARSA, Q-learning이라고 한다."
      ],
      "metadata": {
        "id": "FFPQ8_4AB1o4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 새 섹션ㅇㄹ"
      ],
      "metadata": {
        "id": "AmTVJoCbrGtu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "논문에서 제시하는 기존 접근방법들의 문제점 딥러닝의 도입, 딥러닝 도입의 어려움.\n",
        "\n",
        "1.고차원 입력 즉 큰 상태공간에서 에이전트를 훈련시키는 것은 강화학습의 오랜 도전과제 주 하나이며 이전의 접근방식들은 인간이 직접 유도한 특성을 사용함으로써 이 특성들의 질에 심하게 의존하는 경향이 있다.\n",
        "\n",
        "하지만 딥러닝이 발전하며 이해하기힘든 고차원 데이터에서 효율적으로 특서을 추출할 수 있게 되었고 이를 응용해 위에서 제기한 도전과제를 해결하고자 한다.\n",
        "\n",
        "하지만 딥러닝은 많은양의 라벨링된 데이터가 필요한 반면 강화학습은 라벨링된 데이터 없이 희박하거나 noisy하고 지연된 보상으로부터 학습해야하고 데이터가 독립적으로 가정하는 딥러닝과 달리 강화학습은 상관관계가 큰 sequence를 사용하고 강화학습의 데이터분포는 고정된 딥러닝의 경우와 다르게 행동정책이 업데이트됨에따라 달라지기 때문에 딥러닝 기법을 강화학습에 적용하는데 어려움이 있다.\n",
        "\n",
        "변형된 Q러닝 알고리즘을 사용한 DQN은 이러한 도전과제를 성공적으로 극복했다.\n",
        "\n",
        "확률적 경사하강법을 사용해 weight를 업데이트함으로써 데이터의 상관성과 비정상적인 데이터 분포를 극복했다. 또 경험 재생 매커니즘을 사용함을써 훈련 분포를 부드럽게했다.\n",
        "\n",
        "MDP를 적용하기위한 기반\n",
        "\n",
        "DQN은 에이전트에게 아타리게임을 훈련시키며 입력으로는 오직 게임화면만 제공받는다. 하지만 한시점의 화면만으로는 현재상황에대해 완전히 이해하는것이 불가능하므로(오브젝트가 어디로 이동하고 있는지 등) 하나의 상태를 여러개의 액션과 상태의 묶음으로 정의한다. 이로써 상태에대한 완전한 관찰과 기본적인 마르코프 성질을 충족시킨다.\n",
        "\n",
        "DQN에서 에이전트의 학습목적은 앞으로의 보상을 maximize하게 만드는 action을 선택하는 것 이며 이는 곧 optimal q 그러니까 $Q^*(s,a) = max_\\pi E[R_t|s_t = s, a_t = a, \\pi]$를 찾는것을 목표로 한다는 것 이다.\n",
        "\n",
        "이는 Bellman equation으로 표현 가능하며 미래의 선택이 optimal한 정책에 의해 이루어 졌다면 현상태의 Q값을 maiximize하는 정책으로 다음시점의 Q값중 가장 큰값을 선택하는 전략을 세울 수 있다.\n",
        "\n",
        "$ Q^*(s,a) = E_{s' - \\varepsilon}[r + \\gamma max_{a'}Q^*(s',a')|s,a] $\n",
        "\n",
        "많은 강화학습 알고리즘에서 이를 반복적으로 업데이트함으로써 Q를 추정한다. 하지만 이는 전혀 실용적이지 않은데 그 이유는 Q값을 각 상태,행동 쌍에대해 따로따로 추정하고 일반화가 전혀이루어지지 않기때문에 상태,행동 쌍이 거의 무한한 환경에서는 사용이 불가능 하기 때문이다.\n",
        "\n",
        "DQN에서는 이 문제를 해결하기위해 action value function을 근사하는 $Q(s,a;\\theta) \\approx Q^*(s,a)$를 사용하며 이를 Q-network라고 한다.\n",
        "\n",
        "Q네트워크를 훈련하기위해 사용하는 Loss function은 $L_i(\\theta_i) = E_{s, a-p(.)}[(y_i - Q(s,a;\\theta_i))^2]$ 이고 여기서 $ y_i = E_{s'-\\varepsilon}[r + \\gamma max_{a'}Q(s', a';\\theta_{i-1})] $이다 여기서 중요한 지점은 $ \\theta_{i-1} $이다. Q-network가 업데이되는동안 TD 타겟은 업데이트돼지않고 고정된 파라미터로 샘플링된다. 이는 학습에 사용될 데이터의 분포가 파라미터에 의존하기 때문이다.\n",
        "\n",
        "DQN 아키텍쳐의 특징\n",
        "\n",
        "epcperience replay를 활용한다. 이는 에이전트의 경험 $e_t = (s_t,a_t,r_t,s_{t+1})$을 replay memory D에 저장하며 큐네트워크를 업데이트할때 D에서 과거경험들을 랜덤하게 샘플링해 사용한다.\n",
        "\n",
        "이 접근방식의 장점은 첫째로 각 스텝의 경험이 잠재적으로 많은 웨이트 업데이트에 사용됨으로써 데이터 효율성을 그대화한다\n",
        "\n",
        "두번쨰로는 연이은 샘들들을 학습시키는것은 두 데이터 샘플의 강한 상관관계때문에 비효율적일 수 있다. 하지만 이를 랜덤 샘플링으로 극복한다.\n",
        "\n",
        "세번째로 온폴리시 학습의 단점을 극복할 수 있다. 온폴리시 학습시 행동정책과 타겟정책이 같기때문에 학습이 편향될 수 있지만 경험재생을 통해 모델을 업데이트할때 사용되는 Loss function을 살펴보면 on polcy 학습방법을 사용함을 알 수있고 행동 분포는 미니배치를 사용함을써 평균화되어, 학습을 안정화시키고 파라미터의 진동이나 발산을 방지하 수 있다.\n",
        "\n",
        "이 방법의 문제점도 있는데 과거경험을 무작위샘플링하는 방법과 메모리가 유한하므로 오래된 경험부터 지워나가는 방식은 모두 각 경험의 중요도 모두 같은 지점에 놓는다는 점이다."
      ],
      "metadata": {
        "id": "dlv_npD7rL62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 코드"
      ],
      "metadata": {
        "id": "kKtPc6uyIGml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]\n",
        "!pip install imageio\n",
        "!pip install imageio-ffmpeg"
      ],
      "metadata": {
        "id": "JXtaiWiWOZuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from gymnasium.wrappers import FrameStack, GrayScaleObservation\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "80BJR-O-M7gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"DonkeyKong\" 게임 환경 생성\n",
        "env = gym.make('ALE/DonkeyKong-v5', render_mode='rgb_array')\n",
        "\n",
        "# 정의된 환경에 Frame stack, gray scale warraps를 적용. 순서가 바뀌면 오류가 생김.\n",
        "env = GrayScaleObservation(env)\n",
        "env = FrameStack(env, 4)"
      ],
      "metadata": {
        "id": "ymma95wFOZQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "metadata": {
        "id": "6-EmDuYI42q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space"
      ],
      "metadata": {
        "id": "8W04UgowhKcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()"
      ],
      "metadata": {
        "id": "jZTecMbH7FPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(obs.shape)\n",
        "print(info)\n",
        "obs[0]"
      ],
      "metadata": {
        "id": "sB6kN1vrAykj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경험 리플레이 메모리\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append([state, action, reward, next_state, done])\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(state), action, reward, np.array(next_state), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "0BYqxUInuq0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(state, model, epsilon):\n",
        "    if random.random() > epsilon:\n",
        "        with torch.no_grad():\n",
        "            q_values = model(state)\n",
        "            action = q_values.max(1)[1].item()\n",
        "    else:\n",
        "        action = random.randrange(env.action_space.n)\n",
        "    return action"
      ],
      "metadata": {
        "id": "BSxlSSd9uyoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DQN model architecture 정의"
      ],
      "metadata": {
        "id": "DH9PY3Fzhg6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 논문과 다른점.\n",
        "\n",
        "i) 논문에 명시된 DQN아키텍쳐는 게임이미지를 그레이스케일로 변환하 스택하는 것 말고도 84*84로 크롭핑하는 전처리과정을 포함하고 있지만 여기서는 그냥 Gymnasium 라이브러리에서 반화하는 게임화면을 그레이스케이로 변환해 스택하는 과정만 포함해 원본 이미지 해상도를 그대로 사용한다.\n",
        "\n",
        "ii) Bathch normalization 레이어 사용.\n",
        "\n",
        "게임화면을 원본그대로 사용함에 따라 커볼루션 레이이어 1층, dense레이어 1층이 추가 됨으로 모델이 깊어짐에 따라 컨볼루션레이어에 사이에 bach normalization 레이어를 추가했다.\n",
        "\n",
        " *차이점 관찰 필요.\n",
        "\n",
        "2. Note\n",
        "\n",
        "DQN 아키텍쳐에서 maxpooling 레이어를 사용하지 않는 이유.\n",
        "\n",
        "게임환경에서는 작은 이미자 하나가 중요한 정보를 포함할 확률이 있으므로 정보손실을 야기하는 maxpooling 레이어를 DQN아키텍쳐에 사용하는것은 안 좋은 영향이 있을 수 있다."
      ],
      "metadata": {
        "id": "LCc0cuOrhliG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(4, 16, kernel_size = 8, stride = 4),\n",
        "                                   nn.BatchNorm2d(16),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size = 4, stride = 2),\n",
        "                                   nn.BatchNorm2d(32),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size = 4, stride = 2),\n",
        "                                   nn.BatchNorm2d(64),\n",
        "                                   nn.ReLU())\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(5632, 256),\n",
        "                                nn.ReLU(),\n",
        "                                nn.Linear(256, 18))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = torch.flatten(x,start_dim=1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "lcjpU-6qbCJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN()"
      ],
      "metadata": {
        "id": "7b-Tg_JkdOdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"DonkeyKong\" 게임 환경 생성\n",
        "env = gym.make('ALE/DonkeyKong-v5', render_mode='rgb_array')\n",
        "\n",
        "# 정의된 환경에 Frame stack, gray scale warraps를 적용. 순서가 바뀌면 오류가 생김.\n",
        "env = GrayScaleObservation(env)\n",
        "env = FrameStack(env, 4, )"
      ],
      "metadata": {
        "id": "-3roZ96osL1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs, info = env.reset()"
      ],
      "metadata": {
        "id": "KFe7XIbYuoR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "id": "AqSWLnqdV4rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def totensor(obs):\n",
        "\n",
        "    numpy_array = np.array(obs)  # LazyFrames 객체를 np.array()를 사용하여 NumPy 배열로 변환\n",
        "    tensor = torch.from_numpy(numpy_array) # NumPy 배열을 PyTorch 텐서로 변환\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    tensor = tensor.float()\n",
        "\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "b4EKy7Mpu7R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs = totensor(obs)"
      ],
      "metadata": {
        "id": "RSoRpodWvpYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obs.shape"
      ],
      "metadata": {
        "id": "Z2Alp6RLvvdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qs = model(obs)"
      ],
      "metadata": {
        "id": "wYGQAIx_v573"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(obs, model, epsilon):\n",
        "    if random.random() > epsilon:\n",
        "        with torch.no_grad():\n",
        "            obs = np.array(obs)  # LazyFrames 객체를 np.array()를 사용하여 NumPy 배열로 변환\n",
        "            obs = torch.from_numpy(obs) # NumPy 배열을 PyTorch 텐서로 변환\n",
        "            obs = obs.unsqueeze(0)\n",
        "            obs = obs.float()\n",
        "            q_values = model(obs)\n",
        "            action = q_values.max(1)[1].item()\n",
        "    else:\n",
        "        action = random.randrange(env.action_space.n)\n",
        "    return action"
      ],
      "metadata": {
        "id": "sWelWHXCwDdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action = epsilon_greedy(obs, model, 0.7)"
      ],
      "metadata": {
        "id": "xSphL5zzwJoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "action"
      ],
      "metadata": {
        "id": "8Eo0SuJrwikY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nobs, reward, terminated, truncated, info = env.step(action)"
      ],
      "metadata": {
        "id": "7B8xh1NCwq54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경험 리플레이 메모리\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, reward, next_state):\n",
        "        self.buffer.append([state, action, reward, next_state])\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(state), action, reward, np.array(next_state), done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "aLCeqkwbwtpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# # 언더샘플링\n",
        "\n",
        "# # 인스턴스 생성.\n",
        "# short_term_dataset = CustomDataset(short_term_data, short_term_label)"
      ],
      "metadata": {
        "id": "536Rhs1VajlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \"DonkeyKong\" 게임 환경 생성\n",
        "env = gym.make('ALE/DonkeyKong-v5', render_mode='rgb_array')\n",
        "\n",
        "# 정의된 환경에 Frame stack, gray scale warraps를 적용. 순서가 바뀌면 오류가 생김.\n",
        "env = GrayScaleObservation(env)\n",
        "env = FrameStack(env, 4)\n",
        "\n",
        "model = DQN()\n",
        "\n",
        "# 리플레이버퍼 생성\n",
        "replaybuffer = ReplayBuffer(10000)\n",
        "\n",
        "for episode in range(1000):\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    while(terminated == False and truncated == False):\n",
        "\n",
        "        action = epsilon_greedy(obs, model, 0.5)\n",
        "        nobs, reward, terminated, truncated, info = env.step(action)\n",
        "        replaybuffer.push(totensor(obs), action, reward, totensor(nobs))\n",
        "        obs = nobs\n",
        "\n",
        "    if len(replaybuffer) > 2000:\n",
        "\n",
        "        replay = list(replaybuffer.buffer)\n",
        "        dataset = CustomDataset(replay)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    break\n"
      ],
      "metadata": {
        "id": "xFcEAzpNxatc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(replaybuffer)"
      ],
      "metadata": {
        "id": "xPn4W83HXFYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "replay = list(replaybuffer.buffer)"
      ],
      "metadata": {
        "id": "KnrdXFG8dJD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_tensors = [item[0] for item in replay]"
      ],
      "metadata": {
        "id": "HLzxSMLTet9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(replay)"
      ],
      "metadata": {
        "id": "eybVaMCJbF5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "cuxJCmVnlUQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataloader:\n",
        "    print(data[3].shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "zV_IM-ZJcCOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 강화학습 에이전트\n",
        "class DQNAgent:\n",
        "    def __init__(self, input_shape, n_actions, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01, lr=1e-4, gamma=0.99, buffer_size=10000, batch_size=32):\n",
        "        self.model = DQN(input_shape, n_actions)\n",
        "        self.epsilon = epsilon  # 탐색 비율\n",
        "        self.epsilon_decay = epsilon_decay  # 탐색 감소율\n",
        "        self.epsilon_min = epsilon_min  # 최소 탐색 비율\n",
        "        self.gamma = gamma  # 할인율\n",
        "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "\n",
        "    def act(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(n_actions)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            q_values = self.model(state)\n",
        "            return np.argmax(q_values.detach().numpy())\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.model(states)\n",
        "        next_q_values = self.model(next_states)\n",
        "        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        next_q_value = next_q_values.max(1)[0]\n",
        "        expected_q_value = rewards + self.gamma * next_q_value * (1 - dones)\n",
        "\n",
        "        loss = self.loss_fn(q_value, expected_q_value.detach())\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "# 메인 학습 루프\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"Kong-v0\")  # 킹콩 환경 초기화\n"
      ],
      "metadata": {
        "id": "OxvfN34gtES5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dqn = DQN()\n",
        "optimizer = optim.Adam(dqn.parameters(), lr=1e-4)\n",
        "\n",
        "# Hyperparameters\n",
        "episodes = 1000\n",
        "epsilon_start = 1.0\n",
        "epsilon_final = 0.01\n",
        "epsilon_decay = 300\n",
        "\n",
        "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1. * frame_idx / epsilon_decay)\n",
        "\n",
        "# Training loop\n",
        "for episode in range(episodes):\n",
        "    obs, info = env.reset()\n",
        "    total_reward = 0\n",
        "    terminated, truncated = False\n",
        "    while not (terminated or truncated): # terminated는 에이전트가 성공이나 실패에 이르렀을경우 참을 반환, truncated는 시간제한 초과시 참을 반환\n",
        "        epsilon = epsilon_by_frame(episode)\n",
        "        action = epsilon_greedy(state, q_network, epsilon)\n",
        "        obs, reward, terminated, truncated, info = env.step(0)\n",
        "        total_reward += reward\n",
        "    print(f\"Episode: {episode}, Total Reward: {total_reward}\")"
      ],
      "metadata": {
        "id": "3l7KTxD8nvFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = env.render()\n",
        "plt.imshow(img)\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "PGWbXKFoxCsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = []  # 이미지를 저장할 리스트\n",
        "env = gym.make('ALE/DonkeyKong-v5', render_mode='rgb_array')\n",
        "obs. info = env.reset()\n",
        "\n",
        "terminated = False\n",
        "reward = 0\n",
        "\n",
        "while(terminated == False and reward == 0):\n",
        "    frames.append(env.render())  # 화면을 RGB 배열로 받아 리스트에 추가\n",
        "    action = env.action_space.sample()  # 무작위 행동 선택\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "frames.append(env.render())\n",
        "env.close()\n",
        "\n",
        "# 동영상 파일로 저장\n",
        "with imageio.get_writer('/content/drive/MyDrive/동영상/my_game.mp4', fps=30) as video:\n",
        "    for frame in frames:\n",
        "        video.append_data(frame)"
      ],
      "metadata": {
        "id": "zxr60NyiPKtM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}