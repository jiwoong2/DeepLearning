{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPLFxE3gMV7Ps0LGtKVQvoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klBia3zcWx_u"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CBOW 모델\n",
        "\n",
        "딥러닝을 사용해서  단어의 벡터를 추론한다."
      ],
      "metadata": {
        "id": "xFJ-F_ETZ7v0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bias가 없음.\n",
        "\n",
        "class MatMul:\n",
        "\n",
        "    def __init__(self, W):\n",
        "\n",
        "        self.params = [W]\n",
        "        self.grads = [np.zeros_like(W)]\n",
        "        self.x = None\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        W, = self.params # 원소가 하나 밖에 없으므로 []를 날리기 위해 , 사용.\n",
        "        out = np.dot(x, W)\n",
        "        self.x = x\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        W, = self.parmams\n",
        "        dx = np.dot(dout, W.T)\n",
        "        dW = np.dot(self.x.T, dout)\n",
        "        self.grads[0][...] = dW # .copy 메서드를 사용할 경우 리스트안의 리스트같은 경우 복사가 돼지 않고 원본도 변경됨. ...을 사용해 깊은? 복사.\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "1YVbsTTeW7NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "QS7NsP5sYWgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 샘플 맥락 데이터\n",
        "\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])"
      ],
      "metadata": {
        "id": "BWvtcqykW8iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 가중치 초기화\n",
        "\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)"
      ],
      "metadata": {
        "id": "O0y4bsYjYrNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 계층 생성\n",
        "\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)"
      ],
      "metadata": {
        "id": "aLuAdgN_Y2P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 순전파\n",
        "\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer0.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "print(s)"
      ],
      "metadata": {
        "id": "QVSBv8t_ZKMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple CBOW\n",
        "좀 더 일반적인 CBOW모델"
      ],
      "metadata": {
        "id": "Yt9vEo_eaX5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f') # 자연어 처리는 계산비용이 크기 때문에 데이터타입을 32비트로 지정\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params # 옆으로 이어 붙임.\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0]) # context 희소표현을 모아놓은 3차원 텐서.\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "metadata": {
        "id": "cH5RkQAjZd1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip-gram 모델"
      ],
      "metadata": {
        "id": "FRDzkY6vffqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSkipGram:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layer = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer1 = SoftmaxWithLoss()\n",
        "        self.loss_layer2 = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.in_layer, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in\n",
        "\n",
        "    def forward(self, contexts, target):\n",
        "        h = self.in_layer.forward(target)\n",
        "        s = self.out_layer.forward(h)\n",
        "        l1 = self.loss_layer1.forward(s, contexts[:, 0])\n",
        "        l2 = self.loss_layer2.forward(s, contexts[:, 1])\n",
        "        loss = l1 + l2\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dl1 = self.loss_layer1.backward(dout)\n",
        "        dl2 = self.loss_layer2.backward(dout)\n",
        "        ds = dl1 + dl2\n",
        "        dh = self.out_layer.backward(ds)\n",
        "        self.in_layer.backward(dh)\n",
        "        return None"
      ],
      "metadata": {
        "id": "7isV-3OtflJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}