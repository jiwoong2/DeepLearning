{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "N3OdynW6QT0-",
        "ICKCsvpWQ01p",
        "nPlE8lFzNlVP",
        "3Hdz0541PS61"
      ],
      "gpuType": "V100",
      "authorship_tag": "ABX9TyOZDwI8utqo3ztOZbVczZo3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Short_term_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module"
      ],
      "metadata": {
        "id": "N3OdynW6QT0-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbgevtwTHYTR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch import nn, optim\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import os\n",
        "from torchsummary import summary\n",
        "from contextlib import redirect_stdout\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(DEVICE)"
      ],
      "metadata": {
        "id": "SIZL8YcT0E9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle"
      ],
      "metadata": {
        "id": "ICKCsvpWQ01p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-7 # -1 인경우 한번에 0또는 1만 출력하는 로컬 미니멈으로 수렴.\n",
        "EPOCH = 200\n",
        "batch_size = 50 #데이터셋이 작은데 배치사이즈가 너무크면 문제가생김. 훈련 loss,와 검증 loss 역전, loss 하락폭 감소등 확인.\n",
        "               #배치사이즈가 크면 gpu에 많은 data를 올릴 수 있어 훈련이 빨라지짐.\n",
        "               #배치사이즈가 굉장히 중요한 역할을 하는듯. 이유를 알아보자.\n",
        "graph_save_path = '/content/drive/MyDrive/Colab Notebooks/graph/1'"
      ],
      "metadata": {
        "id": "vzIoxmLiQ3ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short term model data"
      ],
      "metadata": {
        "id": "0crHQF93NZeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_stock_data(Ticker, peri):\n",
        "\n",
        "    ticker = yf.Ticker(Ticker)\n",
        "    stock_data = ticker.history(period=peri)\n",
        "    stock_data = stock_data.reset_index() # 인덱스 초기화\n",
        "    stock_data.drop(['Date','Dividends','Stock Splits'], axis=1, inplace=True) # 불필요한 data 누락\n",
        "    stock_data = stock_data.to_numpy()\n",
        "    return stock_data\n",
        "\n",
        "tickers = [\"AAPL\", \"MSFT\", \"AMZN\", \"GOOGL\", \"NVDA\", \"META\", \"NFLX\", \"AMD\", \"TSLA\", \"INTC\", \"PYPL\", \"QCOM\", \"V\", \"XOM\", \"DIS\",\n",
        "           \"LRCX\", \"LULU\", \"AMGN\", \"MRNA\", \"BKNG\", \"SBUX\", \"ILMN\", \"ADBE\", \"PEP\", \"XEL\", \"MCHP\", \"TEAM\", \"EA\", \"COST\", \"GILD\",\n",
        "           \"FANG\", \"ANSS\", \"CSGP\", \"PCAR\", \"KDP\", \"WBD\", \"WDAY\"]\n",
        "\n",
        "stock_data_dict = {}\n",
        "\n",
        "for ticker in tickers:\n",
        "    stock_data_dict[ticker] = load_stock_data(ticker, \"25y\")"
      ],
      "metadata": {
        "id": "nQWy2iHTHoud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 로그변환.\n",
        "# logged_close_prices = np.log(apple[:, 0:3])\n",
        "\n",
        "# # 원본 배열에 로그 변환된 'Close' 가격을 다시 넣음\n",
        "# apple[:, 0:3] = logged_close_prices"
      ],
      "metadata": {
        "id": "oPvmHOpJtUGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data, label생성기.\n",
        "# Size:데이터개수.\n",
        "# data:원본 data\n",
        "# interval:시계열길이. ex) 30 -> 30일치 데이터를 보고 다음 5일간의 주가변동을 예측.\n",
        "def data_generator(Size:int, data, interval:int):\n",
        "\n",
        "    idx = np.random.choice(range(interval, len(data)-6), size=Size, replace=False)\n",
        "    idx = np.array(idx)\n",
        "\n",
        "    g_data = []\n",
        "    g_label = []\n",
        "\n",
        "    for i in idx:\n",
        "        gs_data = data[i-interval:i, :]\n",
        "        g_data.append(gs_data)\n",
        "\n",
        "        label = (data[i+6,3] - data[i+1,3]) / data[i+1,3] * 100\n",
        "\n",
        "        if  label >= 5:\n",
        "            g_label.append(3)\n",
        "\n",
        "        elif 0 <= label < 5:\n",
        "            g_label.append(2)\n",
        "\n",
        "        elif -5 <= label < 0:\n",
        "            g_label.append(1)\n",
        "\n",
        "        elif label < -5:\n",
        "            g_label.append(0)\n",
        "\n",
        "    g_data = np.array(g_data).astype(np.float32)\n",
        "    g_label = np.array(g_label)\n",
        "\n",
        "    return g_data, g_label"
      ],
      "metadata": {
        "id": "B3pU86iDIPjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 종목별 데이터를 저장할 딕셔너리\n",
        "stock_data = {}\n",
        "\n",
        "# 샘플링할 개수 목록\n",
        "sample_sizes = [4000, 3000, 2000, 1000]\n",
        "\n",
        "# 각 종목에 대해 데이터 생성\n",
        "for ticker in tickers:\n",
        "    # 가능한 샘플링 개수에 대해 시도\n",
        "    for size in sample_sizes:\n",
        "        try:\n",
        "            data, label = data_generator(size, stock_data_dict[ticker], 20)\n",
        "            stock_data[ticker + \"_short_term_data\"] = data\n",
        "            stock_data[ticker + \"_short_term_label\"] = label\n",
        "            break  # 성공적으로 데이터를 생성했으므로 반복 중지\n",
        "        except Exception as e:\n",
        "            print(f\"{ticker}: {size}개 샘플링 실패 - {e}\")\n",
        "            continue  # 다음 샘플링 개수로 시도"
      ],
      "metadata": {
        "id": "yc3dWncUtFxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_term_data_list = []\n",
        "short_term_label_list = []\n",
        "\n",
        "for ticker in tickers:\n",
        "    short_term_data_list.append(stock_data[ticker + \"_short_term_data\"])\n",
        "    short_term_label_list.append(stock_data[ticker + \"_short_term_label\"])\n",
        "\n",
        "short_term_data = np.concatenate(short_term_data_list, axis=0)\n",
        "short_term_label = np.concatenate(short_term_label_list, axis=0)"
      ],
      "metadata": {
        "id": "LTcTScngv8qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(short_term_label==0)"
      ],
      "metadata": {
        "id": "CPaFoy0-wGo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(short_term_data.shape)\n",
        "print(short_term_label.shape)\n",
        "print(sum(short_term_label == 3))"
      ],
      "metadata": {
        "id": "SpXKfZrhzPNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시: X는 3차원 데이터, y는 라벨\n",
        "# X.shape -> (샘플 수, 높이, 너비)\n",
        "# y.shape -> (샘플 수,)\n",
        "X = short_term_data\n",
        "y = short_term_label\n",
        "\n",
        "# 클래스별 샘플 인덱스 추출\n",
        "class_indices = {label: np.where(y == label)[0] for label in np.unique(y)}\n",
        "\n",
        "# 가장 작은 클래스 크기 결정\n",
        "min_size = min(len(indices) for indices in class_indices.values())\n",
        "\n",
        "# 각 클래스에서 무작위로 샘플 선택\n",
        "under_sampled_indices = np.array([], dtype=int)\n",
        "for indices in class_indices.values():\n",
        "    under_sampled_indices = np.concatenate([under_sampled_indices, np.random.choice(indices, min_size, replace=False)])\n",
        "\n",
        "# 선택된 샘플로 데이터셋 생성\n",
        "short_term_data = X[under_sampled_indices]\n",
        "short_term_label = y[under_sampled_indices]"
      ],
      "metadata": {
        "id": "DDnERetre8nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 표준화.\n",
        "# 주가데이터는 지수적으로 증가하므로 각 샘플마다 따로 표준화함.\n",
        "def data_scaler(data):\n",
        "    for sample in range(data.shape[0]):\n",
        "        scaler = StandardScaler()\n",
        "        std = scaler.fit_transform(data[sample,:,:])\n",
        "        data[sample,:,:] = std\n",
        "\n",
        "data_scaler(short_term_data)"
      ],
      "metadata": {
        "id": "Kp13OIDWKqXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "# 언더샘플링\n",
        "\n",
        "# 인스턴스 생성.\n",
        "short_term_dataset = CustomDataset(short_term_data, short_term_label)\n",
        "\n",
        "#데이터를 훈련 데이터와 검증 데이터, 테스트 데이터로 나누기. 파이토치데이터셋 인스턴스에 사용하면 라벨도 같이 분류됨.\n",
        "#훈련세트 80%, 테스트세트 20% 분류\n",
        "train_size = int(0.8 * len(short_term_dataset))\n",
        "test_size = len(short_term_dataset) - train_size\n",
        "\n",
        "short_term_train_dataset, short_term_test_dataset = random_split(short_term_dataset, [train_size, test_size])\n",
        "\n",
        "#테스트세트를 테스트세트 10%, 검증세트 10% 분류\n",
        "test_size = int(0.2 * len(short_term_test_dataset))\n",
        "val_size = len(short_term_test_dataset) - test_size\n",
        "\n",
        "short_term_test_dataset, short_term_val_dataset = random_split(short_term_test_dataset, [test_size, val_size])\n",
        "\n",
        "# 데이터 로더 생성.\n",
        "short_term_train_DL = DataLoader(short_term_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "short_term_val_DL = DataLoader(short_term_val_dataset, batch_size=batch_size, shuffle=True)\n",
        "short_term_test_DL = DataLoader(short_term_test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 데이터 로더를 통해 데이터 확인\n",
        "for features, labels in short_term_train_DL:\n",
        "    print(features.shape)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "id": "rCSHtapULlqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(short_term_train_DL.dataset))\n",
        "print(len(short_term_test_DL.dataset))\n",
        "print(len(short_term_val_DL.dataset))"
      ],
      "metadata": {
        "id": "A5iNDUbXNNUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(short_term_train_DL.dataset[:][1] == 0)"
      ],
      "metadata": {
        "id": "nuYFeRfwHGIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = [sum(short_term_train_DL.dataset[:][1] == 0), sum(short_term_train_DL.dataset[:][1] == 1), sum(short_term_train_DL.dataset[:][1] == 2), sum(short_term_train_DL.dataset[:][1] == 3)]\n",
        "total = sum(class_counts)\n",
        "class_weights = [total / count for count in class_counts]\n",
        "class_weights = torch.FloatTensor(class_weights)\n",
        "print(class_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "criterion = criterion.to(DEVICE) # loss에 가중치를 부여하는경우 손실함수도 gpu에 올려야함."
      ],
      "metadata": {
        "id": "fUS9tLcFHxO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Short term model"
      ],
      "metadata": {
        "id": "nPlE8lFzNlVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class ShortTermModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.conv1 = nn.Sequential(nn.Conv2d(1, 30, 5, stride=1),\n",
        "#                                    nn.BatchNorm2d(30),\n",
        "#                                    nn.ReLU(),\n",
        "#                                    nn.Dropout2d(0.7))\n",
        "\n",
        "#         self.conv2 = nn.Sequential(nn.Conv2d(1, 60, kernel_size=(30, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(60),\n",
        "#                                    nn.ReLU(),\n",
        "#                                    nn.Dropout2d(0.7))\n",
        "\n",
        "#         self.conv3 = nn.Sequential(nn.Conv2d(1, 120, kernel_size=(60, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(120),\n",
        "#                                    nn.ReLU(),\n",
        "#                                    nn.Dropout2d(0.7))\n",
        "\n",
        "#         self.conv4 = nn.Sequential(nn.Conv2d(1, 240, kernel_size=(120, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(240),\n",
        "#                                    nn.ReLU(),\n",
        "#                                    nn.Dropout2d(0.7))\n",
        "\n",
        "#         self.linear = nn.Sequential(nn.Linear(960,100), nn.GELU(),\n",
        "#                                     nn.Dropout(0.7),\n",
        "#                                     nn.Linear(100,4))\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = x.permute(0,2,1)\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = self.conv1(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv2(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv3(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv4(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.linear(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# model = ShortTermModel().to(DEVICE)\n",
        "# optimizer = optim.Adam(model.parameters(), lr = LR)"
      ],
      "metadata": {
        "id": "m8zQZ1dANn2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ShortTermModel(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.conv1 = nn.Sequential(nn.Conv2d(1, 5, 5, stride=1),\n",
        "#                                    nn.BatchNorm2d(5),\n",
        "#                                    nn.GELU())\n",
        "\n",
        "#         self.conv2 = nn.Sequential(nn.Conv2d(1, 10, kernel_size=(5, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(10),\n",
        "#                                    nn.GELU())\n",
        "\n",
        "#         self.conv3 = nn.Sequential(nn.Conv2d(1, 20, kernel_size=(10, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(20),\n",
        "#                                    nn.GELU())\n",
        "\n",
        "#         self.conv4 = nn.Sequential(nn.Conv2d(1, 40, kernel_size=(20, 5), stride=1),\n",
        "#                                    nn.BatchNorm2d(40),\n",
        "#                                    nn.GELU())\n",
        "\n",
        "#         self.linear = nn.Sequential(nn.Linear(160,4))\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x = x.permute(0,2,1)\n",
        "#         x = x.unsqueeze(1)\n",
        "#         x = self.conv1(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv2(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv3(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = self.conv4(x)\n",
        "#         x = x.squeeze().unsqueeze(1)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.linear(x)\n",
        "\n",
        "#         return x\n",
        "\n",
        "# model = ShortTermModel().to(DEVICE)\n",
        "# optimizer = optim.Adam(model.parameters(), lr = LR)"
      ],
      "metadata": {
        "id": "I97GSLHxmidZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class RNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.rnn = nn.RNN(5,30,3,batch_first = True)\n",
        "#         self.linear = nn.Linear(30,4)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _, x = self.rnn(x)\n",
        "#         x = x[-1,:,:]\n",
        "#         # x = x.squeeze(0)\n",
        "#         x = self.linear(x)\n",
        "#         return x\n",
        "\n",
        "# model = RNN().to(DEVICE)\n",
        "# optimizer = optim.Adam(model.parameters(), lr = LR)"
      ],
      "metadata": {
        "id": "cVoPIbEtvJaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(5, 30, 3, batch_first=True)\n",
        "\n",
        "        # 선형 레이어 정의\n",
        "        self.linear = nn.Linear(30, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # LSTM 레이어를 통과\n",
        "        _, x = self.lstm(x) #out1은 각 시간의 출력값, out2는 마지막시간의 히든과 셀상태의 튜플로 반환됨.\n",
        "\n",
        "        # out2[0][-1,:,:] out2[0]은 마지막 시간의 히든상태. 뒤의 슬라이싱은 스택된 레이어중 가장마지막레이어의\n",
        "        # 히든상태를 반환.\n",
        "        # 마지막 시간 단계의 출력을 선형 레이어로 전달\n",
        "        x = self.linear(x[0][-1,:,:])\n",
        "        return x\n",
        "\n",
        "\n",
        "model = LSTM()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr= LR)"
      ],
      "metadata": {
        "id": "scJ0xx-0T03w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in short_term_train_DL:\n",
        "#     test_data = i[0]\n",
        "#     break\n",
        "\n",
        "# test_data.shape"
      ],
      "metadata": {
        "id": "waKQ5sP4mqHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = LSTM()\n",
        "# a = model(test_data)\n",
        "# print(a.shape)"
      ],
      "metadata": {
        "id": "IfUDtUQGmqhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "3Hdz0541PS61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1에폭 트레이닝.\n",
        "def loss_epoch(model, DL, criterion, optimizer = None):\n",
        "\n",
        "    N = len(DL.dataset) # the number of data\n",
        "    rloss = 0; rcorrect = 0\n",
        "\n",
        "    for x_batch, y_batch in DL:\n",
        "\n",
        "        x_batch = x_batch.to(DEVICE)\n",
        "        y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "        # inference\n",
        "        y_hat = model(x_batch)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(y_hat,y_batch)\n",
        "\n",
        "        # update\n",
        "        if optimizer is not None:\n",
        "            optimizer.zero_grad() # gradient 누적을 막기 위한 초기화\n",
        "            loss.backward() # backpropagation\n",
        "            optimizer.step() # weight update\n",
        "\n",
        "        # loss accumulation\n",
        "        loss_b = loss.item() * x_batch.shape[0] # batch loss # BATCH_SIZE를 곱하면 마지막 18개도 32개를 곱하니까..\n",
        "        rloss += loss_b # running loss\n",
        "        # accuracy accumulation\n",
        "        pred = y_hat.argmax(dim=1)\n",
        "        corrects_b = torch.sum(pred == y_batch).item()\n",
        "        rcorrect += corrects_b\n",
        "\n",
        "    loss_e = rloss/ N # epoch loss\n",
        "    accuracy_e = rcorrect/N*100\n",
        "\n",
        "    return loss_e, accuracy_e, rcorrect\n",
        "\n",
        "def Train(model, train_DL, val_DL, criterion, optimizer, EPOCH):\n",
        "\n",
        "    loss_history = {\"train\":[], \"val\":[]}\n",
        "    acc_history = {\"train\":[], \"val\":[]}\n",
        "\n",
        "    for ep in tqdm(range(EPOCH), leave=False):\n",
        "\n",
        "        model.train() # train mode로 전환\n",
        "        train_loss, train_acc, _ = loss_epoch(model, train_DL, criterion, optimizer)\n",
        "        loss_history[\"train\"] += [train_loss]\n",
        "        acc_history[\"train\"] += [train_acc]\n",
        "\n",
        "        model.eval() # test mode로 전환\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_acc, _ = loss_epoch(model, val_DL, criterion)\n",
        "            loss_history[\"val\"] += [val_loss]\n",
        "            acc_history[\"val\"] += [val_acc]\n",
        "\n",
        "        # # print loss\n",
        "        # print(f\"train loss: {round(train_loss,5)}, \"\n",
        "        #       f\"val loss: {round(val_loss,5)} \\n\"\n",
        "        #       f\"train acc: {round(train_acc,1)} %, \"\n",
        "        #       f\"val acc: {round(val_acc,1)} %, time: {round(time.time()-epoch_start)} s\")\n",
        "        # print(\"-\"*20)\n",
        "\n",
        "    return loss_history, acc_history\n",
        "\n",
        "def Test(model, test_DL, criterion):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_loss, test_acc, rcorrect = loss_epoch(model, test_DL, criterion)\n",
        "    print()\n",
        "    print(f\"Test loss: {round(test_loss,5)}\")\n",
        "    print(f\"Test accuracy: {rcorrect}/{len(test_DL.dataset)} ({round(test_acc,1)} %)\")\n",
        "    return round(test_acc,1)\n",
        "\n",
        "def graph(loss, acc, EPOCH, batch_size, LR, graph_save_path=None):\n",
        "    # 두 개의 그래프를 하나의 figure에 가로로 배열\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "    # 첫 번째 그래프 (Train Loss와 Validation Loss)\n",
        "    axs[0].plot(range(1, EPOCH + 1), loss['train'], label='Train Loss')\n",
        "    axs[0].plot(range(1, EPOCH + 1), loss['val'], label='Validation Loss')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_ylabel('Loss')\n",
        "    axs[0].set_title(f'LOSS GRAPH    LR:{LR}, EPOCH:{EPOCH}, Batch size:{batch_size}')\n",
        "    axs[0].legend()\n",
        "\n",
        "    # 두 번째 그래프 (Train Accuracy와 Validation Accuracy)\n",
        "    axs[1].plot(range(1, EPOCH + 1), acc['train'], label='Train acc')\n",
        "    axs[1].plot(range(1, EPOCH + 1), acc['val'], label='Validation acc')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_ylabel('Accuracy')\n",
        "    axs[1].set_title(f'AC GRAPH    LR:{LR}, EPOCH:{EPOCH}, Batch size:{batch_size}')\n",
        "    axs[1].legend()\n",
        "\n",
        "    # 그래프 출력\n",
        "    plt.tight_layout()  # 그래프 간격 조정\n",
        "\n",
        "    # 그래프를 이미지 파일로 저장\n",
        "    if graph_save_path:\n",
        "        plt.savefig(graph_save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def test_matrix(model, DL, matrix_save_path=None):\n",
        "    model.eval()  # 평가 모드로 설정\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "\n",
        "    model.to('cpu')\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
        "        for inputs, labels in DL:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            y_pred.extend(predicted.numpy())  # 예측값 저장\n",
        "            y_true.extend(labels.numpy())    # 실제 레이블 저장\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d')\n",
        "    plt.ylabel('lable')\n",
        "    plt.xlabel('predict')\n",
        "\n",
        "    # 그래프를 이미지 파일로 저장\n",
        "    if matrix_save_path:\n",
        "        plt.savefig(matrix_save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "k2AJ4f1FOEP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experience"
      ],
      "metadata": {
        "id": "ULg3oDhcPbKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for LR in [5e-5]:\n",
        "    for epoch in [500]:\n",
        "        for batch in [150]:\n",
        "\n",
        "            #모델 초기화\n",
        "            model = LSTM().to(DEVICE)\n",
        "\n",
        "            #배치사이즈 설정\n",
        "            short_term_train_DL = DataLoader(short_term_train_dataset, batch_size=batch, shuffle=True)\n",
        "            short_term_val_DL = DataLoader(short_term_val_dataset, batch_size=batch, shuffle=True)\n",
        "            short_term_test_DL = DataLoader(short_term_test_dataset, batch_size=batch, shuffle=True)\n",
        "\n",
        "            #생성된 데이터로더의 업데이트가중치\n",
        "            class_counts = [sum(short_term_train_DL.dataset[:][1] == 0), sum(short_term_train_DL.dataset[:][1] == 1), sum(short_term_train_DL.dataset[:][1] == 2), sum(short_term_train_DL.dataset[:][1] == 3)]\n",
        "            total = sum(class_counts)\n",
        "            class_weights = [total / count for count in class_counts]\n",
        "            class_weights = torch.FloatTensor(class_weights)\n",
        "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "            criterion = criterion.to(DEVICE) # loss에 가중치를 부여하는경우 손실함수도 gpu에 올려야함.\n",
        "\n",
        "            #옵티마이저 재설정\n",
        "            optimizer = optim.Adam(model.parameters(), lr = LR)\n",
        "\n",
        "            #학습\n",
        "            loss, acc = Train(model, short_term_train_DL, short_term_val_DL, criterion, optimizer, epoch)\n",
        "\n",
        "            # 새 폴더 생성 경로 설정\n",
        "            folder_path = f'/content/drive/MyDrive/Colab Notebooks/graph/LSTMunder3{LR}{epoch}{batch}'\n",
        "            if not os.path.exists(folder_path):\n",
        "                os.makedirs(folder_path)\n",
        "\n",
        "            # 파일 저장 경로 설정\n",
        "            file_path = os.path.join(folder_path, 'model_summary.txt')\n",
        "\n",
        "            # 요약된 모델 아키텍처를 직접 파일에 리디렉션합니다.\n",
        "            # LSTM레이러는 요약이 안됨. 튜플이 반환돼는게 문제인듯.\n",
        "            # with open(file_path, 'w') as f:\n",
        "            #     with redirect_stdout(f):\n",
        "            #         summary(model, (30, 5))\n",
        "\n",
        "            #결과그래프\n",
        "            graph(loss, acc, epoch, batch, LR, folder_path+'/graph.png')\n",
        "\n",
        "            #혼동행렬\n",
        "            test_matrix(model, short_term_test_DL, folder_path+'/matrix.png')"
      ],
      "metadata": {
        "id": "Hfuz7vVI1o3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(DEVICE)\n",
        "Test(model, short_term_test_DL, criterion)"
      ],
      "metadata": {
        "id": "33Nj2rwjRKKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sum(short_term_test_DL.dataset[:][1]==0)/short_term_test_DL.dataset[:][1].size*100)\n",
        "print(sum(short_term_test_DL.dataset[:][1]==1)/short_term_test_DL.dataset[:][1].size*100)\n",
        "print(sum(short_term_test_DL.dataset[:][1]==2)/short_term_test_DL.dataset[:][1].size*100)\n",
        "print(sum(short_term_test_DL.dataset[:][1]==3)/short_term_test_DL.dataset[:][1].size*100)"
      ],
      "metadata": {
        "id": "3hvaOAQ5pL3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model save"
      ],
      "metadata": {
        "id": "RceLm2HPXUkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#전체 모델 저장하기\n",
        "#torch.save(model, '/content/drive/MyDrive/Colab Notebooks/results/short_term_model.pt')\n",
        "\n",
        "# # 나중에 전체 모델 로드하기\n",
        "# model = torch.load('model_complete.pth')\n",
        "# model.eval()  # 추론 모드로 설정"
      ],
      "metadata": {
        "id": "lCDhCuHaXXO4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}