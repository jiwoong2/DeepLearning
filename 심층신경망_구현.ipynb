{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7gqKiGwEL_bz"
      ],
      "authorship_tag": "ABX9TyPqaJQL2F7yZmccJTTd+GCP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/%EC%8B%AC%EC%B8%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D_%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUfvI_TQ9saa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import OrderedDict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layers"
      ],
      "metadata": {
        "id": "7gqKiGwEL_bz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4차원 텐서를 염두해둔 코드이기 떄문에 다음 학기때완전히 이해가능\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W # 가중치 행렬\n",
        "        self.b = b # bias 벡터\n",
        "\n",
        "        # 입력된 data 초기화\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "        # 가중치와 bias 미분\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 텐서 대응\n",
        "        self.original_x_shape = x.shape\n",
        "        x = x.reshape(x.shape[0], -1) # 4차원 텐서를 행렬로 변환. -1을 입력하면 reshape 가능한 숫자를 알아서 대입한다.\n",
        "        self.x = x\n",
        "\n",
        "        out = np.dot(self.x, self.W) + self.b # affine 변환. bias의 경우 배치처리를 할경우 행렬뎃섬에서 shape이 맞지 않는데 numpy에서 자동으로 벡터를 복사해 각 열에 더하게 된다.\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout): # dout:흘러들어온 미분\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0) # 행렬의 경우 axis 0: 행, 1: 열. 텐서의 경우 axis 0: 각 행렬, 1: 각 행렬의 행, 2: 각 행렬의 열\n",
        "\n",
        "        dx =dx.reshape(*self.original_x_shape) # 입력데이터\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "4xZ7JlLhMDXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "        return dx"
      ],
      "metadata": {
        "id": "e1ZxpHMUMKJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0) # self.mask는 0보다 크면 false 0보다 작으면 true인 리스트를 반환한다.\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0 # 설명1\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0 # 위의 설명1을 참조. 주의할점은 흘러들어온 미분값의 부호에 의해 1이나 0을 곱하는것이 정해지는게 아니라 위의 x값을 기준으로 gradient를 살리거나 죽이게 된다.\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "AXRYLDc8MNJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1: # y가 벡터인 경우, 배치처리를 안 한 경우\n",
        "        t = t.reshape(1, t.size) # 10차원 벡터를 행렬로 변환.(개념상.)\n",
        "        y = y.reshape(1, y.size)\n",
        "\n",
        "    if t.size == y.size: # 라벨이 원-핫 인코딩이 돼어 있다면, 그러니까 입력 값이 10차원 벡터라면, size함수는 메트릭스의 원소갯수를 반환한다.\n",
        "        t = t.argmax(axis=1) # 원-핫 인코딩 이전으로 되돌리기 위해 작성.\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size # 설명1, 확률벡터로 이루어진 행렬에서 알맞은 인덱스를 얻기위한 과정.\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:   # x가 행렬일 경우, 즉 배치처리를 해서 각 행이 확률벡터인 메트릭스를 입력받은 경우.\n",
        "        x = x.T # 각 행이 확률벡터인 초기 메트릭스에서 각 열이 확률벡터인 메트릭스로 전치 시킨다.\n",
        "        x = x - np.max(x, axis = 0) # np.max(x, axis = 0)는 각 열에 대한 최댓값을 저장한 리스트(벡터)를 반환 한다. 그리고 x에 대한 - 연산은 각 열의 확률벡터에 각 열의 최댓값을 빼주는 연산이다.(밑의 오버플로우 방지를 위한 최댓값 빼기와 같음.)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis = 0) # 위와 같은 행렬과 벡터의 연산 원리.( 밑의 예제 참고.)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x) # 오버플로를 방지하기위해 작성. 노트참고. , vector에 scalar를 빼면 numpy에서 알아서 원소별로 연산을 한다.\n",
        "    return np.exp(x) / np.sum(np.exp(x))  # normalize\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None #손실함수\n",
        "        self.y = None # softmax의 출력(확률 벡터)\n",
        "        self.t = None # 정답 레이블(one-hot encoding 형태)\n",
        "\n",
        "    def forward(self, x,  t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout = 1): # 역전파의 시작은 1 이다.\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩이 돼어 있는 경우.\n",
        "            dx = (self.y - self.t) / batch_size # 손실함수의 정의 값은 각각의 손실함수값의 평균이므로 배치사이즈로 나누어 준다.\n",
        "\n",
        "        else: # 원-핫 이코딩이 돼어 있지 않은 경우.\n",
        "            dx = self.y.copy()\n",
        "            dx[np.arange(batch_size), self.t] -= 1 # 도출된 확률벡터는 배치처리로 묶여있다는 것을 생각해보면 위와 같은 결과를 도출함.\n",
        "            dx = dx / batch_size\n",
        "\n",
        "            return dx"
      ],
      "metadata": {
        "id": "AjMY1HAxMPl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchNormalization:\n",
        "\n",
        "    def __init__(self, gamma, beta, momentum = 0.9, running_mean = None, running_var = None): # gamma와 beta는 머신이 학습해 최적값을 찾지만 초기값을 설정해 줘야 한다.\n",
        "                                                                                              # running_mean = None, running_var = None : 학습중엔 각 배치행렬(data 묶음)의 열(feature)을 사용해\n",
        "                                                                                              # 평균과, 분산을 구해 사용하지만 학습이 끝난후엔 학습하는 동안의 전체 훈련용 data의 평균과 분산(비슷한 값?)\n",
        "                                                                                              # 을 사용해 normalizetion 한다.\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.input_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
        "\n",
        "        # test용 평균, 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var\n",
        "\n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg = True): # x는 입력 data, train_flag : 위의 주석처럼 훈련시와 test시의 순전파방식이 다르기 때문에 훈련시 train_flg를 True로 지정해 이를 구분한다.\n",
        "        self.input_shape = x.shape\n",
        "        if x.ndim != 2: # 행렬이 아니면. 지금까지는 data를 flatten한후 배치처리 했기때문에 인풋이 행렬이었지만, cnn등에서는 data를 faltten시키지 않기때문에 행렬이 아니라 텐서가 입력된다.\n",
        "                        # N은 배치사이즈, C는 채널, H는 세로해상도, W는 가로해상도\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "\n",
        "        return out.reshape(*self.input_shape) # shape을 원래대로 복원한다. *는 ()를 없에준다.\n",
        "\n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = np.zeros(D)\n",
        "            self.running_var = np.zeros(D)\n",
        "\n",
        "        if train_flg: # 훈련시.\n",
        "            mu = x.mean(axis = 0) # 열의 평균\n",
        "            xc = x - mu # 각 열의 평균이 0이 됨.\n",
        "            var = np.mean(xc**2, axis = 0) # 분산\n",
        "            std = np.sqrt(var + 10e-7) # 표준편차\n",
        "            xn = xc / std # 각 열의 표준편차가 1이됨.\n",
        "\n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu # RMSProp에서 사용한 기법처럼 평균벡터를 계속해서 내분하는 과정. 결과적으로 계속해서 바뀌는 배치묶음의 평균 추세를 따라간다.\n",
        "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
        "\n",
        "        else: # train_flg가 false일 경우 즉, test시에는 running_mean와 running_var을 사용한다.\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
        "\n",
        "        out = self.gamma * xn + self.beta # gamma를 곱해 표준편차를 조정하고 beta를 더해 평균을 조정한다.\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.input_shape)\n",
        "\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis = 0) # 덧셈노드를  브로드캐스팅(repeat 노드를 통과)방식으로 계산하기때문에 덧셈노드의 역전파뿐 아니라 repeat노드의 역전파인 sum노드를 추가해야한다.\n",
        "        dgamma = np.sum(self.xn * dout, axis = 0) # *는 adamard  product임.\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis = 0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = np.sum(dxc, axis = 0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "\n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "zB9kn9VqMSgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dropout:\n",
        "\n",
        "    def __init__(self, dropout_ratio = 0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg = True): # Dropout층은 훈련시에만 적용함.\n",
        "        if train_flg:\n",
        "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio # .rand는 0부터 1까지의 균등분포로 랜덤값을 생성함. self.mask는 true, false로 이루어진 행렬이 됨.\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio) # 신호량을 맞추기위해 리스케일링. 이해안감.\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask # 신호를 죽인부분은 미분도 죽임."
      ],
      "metadata": {
        "id": "LugoOA7jMXER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 심층신경망"
      ],
      "metadata": {
        "id": "EFBV2bP4L64d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size_list, output_size, activation = 'relu', weight_init_std = 'relu',  # weight_init_std : 가중치의 표준편차 지정\n",
        "                 weight_decay_lambda = 0, use_dropout = False, dropout_ration = 0.5, use_bachnorm = False):       # relu나 he로 지정하면 HE초기값으로, sigmoid나 xaviar로 지정하면 xaviar초기값으로 설정\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.weight_decay_lambda = weight_decay_lambda\n",
        "        self.use_bachnorm = use_bachnorm\n",
        "        self.params = {}\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.__init_weight(weight_init_std)\n",
        "\n",
        "        # 계층 생성\n",
        "        activation_layer = {'sigmoid' : Sigmoid, 'relu' : Relu}\n",
        "\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num + 1):\n",
        "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "            if self.use_bachnorm: # batchnormalization층을 사용할 경우.\n",
        "                self.params['gamma' + str(idx)] = np.ones(hidden_size_list[idx - 1]) # scail의 초기값은 1로지정.(변화없음)\n",
        "                self.params['beta' + str(idx)] = np.zeros(hidden_size_list[idx - 1]) # shift의 초기값은 0으로 지정.(변화없음)\n",
        "                self.layers['BatchNorm'+ str(idx)] = BatchNormalization(self.params['gamma' + str(idx)], self.params['beta' + str(idx)]) # 레이어 추가\n",
        "\n",
        "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]() # 인스턴스 생성\n",
        "\n",
        "            if self.use_dropout:\n",
        "                self.layers['Dropout' + str(idx)] = Dropout(dropout_ration)\n",
        "\n",
        "        idx = self.hidden_layer_num + 1 # 마지막 계층은 activation층이 없기 때문에 따로 생성한다.\n",
        "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def __init_weight(self, weight_init_std):\n",
        "        \"\"\" 가중치 초기화\n",
        "        weight_init_std : 가중치의 표준편차 지정\n",
        "        relu나 he로 지정하면 HE초기값으로 설정, sigmoid나 xavier로 지정하면 Xavier초기값으로 지정\n",
        "        \"\"\"\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size] # 자료형이 numpy array라면 덧셈연산시 벡터연산을하지만 리스트라면 단순히 옆으로 이어붙인다.\n",
        "                                                                                       # mnist 예시에서는 [784, 100, 100, 100, 10]인 리스트가 생성됨\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            scale = weight_init_std\n",
        "            if str(weight_init_std).lower() in ('relu', 'he'): # lower()은 알파벳을 소문자로 바꿔줌.\n",
        "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할때에는 HE초기값을 사용\n",
        "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
        "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할때에는 xabier초기값을 사용\n",
        "\n",
        "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx]) # 784x100행렬을 생성. 원소는 평균이 0 표준편차가 scale인 분포에서 뽑는다.\n",
        "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x, train_flg = False):\n",
        "        for key, layer in self.layers.items(): # Dropout층과 BatchNorm층은 훈련시와 테스트시의 과정이 다르기 때문에  train_flg 매개변수로 구분해 준다.\n",
        "            if 'Dropout' in key or 'BatchNorm' in key:\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t, train_flg = False): # x는 입력데이터 t는 정답 레이블.\n",
        "\n",
        "        y = self.predict(x, train_flg)\n",
        "\n",
        "        weight_decay = 0\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            W = self.params['W' + str(idx)]\n",
        "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2) # 모든 가중치의 제곱을 더한 후 lambda와 1/2를 곱하면 L2-regularization을 구할 수 있다.\n",
        "\n",
        "        return self.last_layer.forward(y, t) + weight_decay # 크로스엔트로피와  L2-regularization을 더해서 최종적인 손실함수 값을 반환한다.\n",
        "\n",
        "    def accuracy(self, X, T):\n",
        "        Y = self.predict(X, train_flg = False)\n",
        "        Y = np.argmax(Y, axis = 1)\n",
        "        if T.ndim != 1 : T = np.argmax(T, axis = 1) # 라벨이 원-핫 인코딩이 돼어 있는 경우\n",
        "\n",
        "        accuracy = np.sum(Y == T) / float(X.shape[0])\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "\n",
        "        # forward\n",
        "        self.loss(x, t, train_flg = True)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W # L2규제에 대한 미분을 더해준다(미분의 Linear한 특성을 이용.)\n",
        "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db # bias 벡터는 L2규제가 매개변수롤 weight만을 포함하고 있기때문에 L2의 미분값이 0이다.\n",
        "\n",
        "            if self.use_bachnorm and idx != self.hidden_layer_num + 1: # 마지막층은 제외하기위해\n",
        "                grads['gamma' + str(idx)] = self.layers['BatchNorm' + str(idx)].dgamma\n",
        "                grads['beta' + str(idx)] = self.layers['BatchNorm' + str(idx)].dbeta\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "EUCiOyEr_4vM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}