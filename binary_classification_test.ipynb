{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7oSt8g7GzHMO"
      ],
      "authorship_tag": "ABX9TyMKt4d2Rp6eGlNoz+gu/zxT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/binary_classification_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델의 depth와 활성함수의 중첩, 그에 따른 비선형성 증가의 의미.\n",
        "\n",
        "딥러닝 모델의 깊어질수록 활성함수가 중첩되고 모델전체의 비선형성이 증가하게 된다. 비선형성이 증가함에따라 모델이 그리는 곡면은 더 복잡해지게 되므로 SGD같이 local minimum을 탈출할 수단이 없는 optimizer는 local minimum에 빠질 확률이 커지게 되고 sigmoid같이 모든지점에서 기울기가 1이하인 activation function을 갖는 모델은 vanishing gradient문제가 생길 것 이다. 이번 프로젝트를 통해 모델의 깊이에따른 학습정체를 확인하고 그 원인이 vanishing gradient때문인지 아니면 local minimum때문인지 분석해본다."
      ],
      "metadata": {
        "id": "kfp1WidqwT8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**가정1 vanishing gradient가 원인인 경우**\n",
        "\\\n",
        "\\\n",
        "실험1\n",
        "\n",
        "활성화 함수로 Sigmoid 함수를 사용할 경우 vanishing gradient로 학습이 제대로 이뤄지지 않을것이다. 반면 gradient를 잘 보존하는 ReLU를 사용할 경우 학습이 잘 이뤄이질 것 이다.\n",
        "\\\n",
        "\\\n",
        "\\\n",
        "**가정2 local minimum이 원인인 경우**\n",
        "\\\n",
        "\\\n",
        "실험1\n",
        "\n",
        "학습정체의 원인이 local minimum인 경우 모델이 완전히 local minimum에 묶였다고 생각될때 gradient값을 확인해 보면 거의 0에 수렴할 것 이다.\n",
        "\\\n",
        "\\\n",
        "조건2 활성함수로 ReLU를 사용할 경우\n",
        "\n",
        "gradient가 잘 보존되어도 sgd특성상 local minimum을 빠져나올 수 없음.\n",
        "\\\n",
        "\\\n",
        "조건3\n",
        "\n",
        "local minimum으로 완전히 수렴한후엔 learning rate를 매우크게 줘도 local munimumdptj 빠져나올 수 없음.\n",
        "\\\n",
        "\\\n",
        "조건4\n",
        "\n",
        "momentum등 local minimum을 탈출할 수 있는 optimizer를 사용하면 최적화 가능."
      ],
      "metadata": {
        "id": "vx7OLp06GXaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data 생성"
      ],
      "metadata": {
        "id": "7oSt8g7GzHMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j6dQVVruCJup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSQnPan6AoTN"
      },
      "outputs": [],
      "source": [
        "# case 1 키, 몸무게\n",
        "N=20\n",
        "random0=torch.randn(int(N/2),1)\n",
        "random5=torch.randn(int(N/2),1)+8\n",
        "\n",
        "class1_data=torch.hstack([random0,random5])\n",
        "class2_data=torch.hstack([random5,random0])\n",
        "# class3_data=torch.hstack([random5,random0+8])\n",
        "\n",
        "class1_label=torch.ones(int(N/2),1)\n",
        "class2_label=torch.zeros(int(N/2),1)\n",
        "class3_label=torch.ones(int(N/2),1)\n",
        "\n",
        "X=torch.vstack([class1_data,class2_data])\n",
        "y=torch.vstack([class1_label,class2_label])\n",
        "\n",
        "# X=torch.vstack([class1_data,class2_data,class3_data])\n",
        "# y=torch.vstack([class1_label,class2_label,class3_label])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(class1_data[:,0],class1_data[:,1],'bo')\n",
        "plt.plot(class2_data[:,0],class2_data[:,1],'ro')\n",
        "# plt.plot(class3_data[:,0],class3_data[:,1],'bo')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "GFZpfnNVCWwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가정1 - 실험1"
      ],
      "metadata": {
        "id": "wpcl0qvGvoyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid와 ReLU비교를 위한 모델 생성"
      ],
      "metadata": {
        "id": "96dvKjcpRTXE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Sequential(nn.Linear(2, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 1), nn.Sigmoid())\n",
        "\n",
        "        self.linear2 = nn.Sequential(nn.Linear(2, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 10), nn.ReLU(),\n",
        "                                    nn.Linear(10, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x1 = self.linear1(x)\n",
        "        x2 = self.linear2(x)\n",
        "\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "OS8Q_N6jCcLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "gradient 관찰.\n",
        "\n",
        "ReLU가 Sigmoid보다 gradient를 잘 보존하지만 ReLU만 사용해서는 vanishing gradient현상을 완전히 극복할수는 없는 것 같다.\n",
        "\\\n",
        "\\\n",
        "! 파라미터가 너무적은(나의 경우 5x5 Linear 레이어)레이어를 깊게 쌓다보면 가끔 ReLU를 사용한 모델의 gradient가 어느순간 0으로 수렴 후 회복하지 못 하는 현상이 생긴다. 음수값에서 gradient를 0으로 반환하는 ReLU함수의 특성으로 생기는 현상인 듯 하다. 학습과정에서 이러한 현상이 생기면 안되므로 각 레이어의 파라미터 수를 10x10으로 수정했다."
      ],
      "metadata": {
        "id": "_E0E-UlARpxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Model1()\n",
        "model1.train()\n",
        "optimizer = optim.SGD(model1.parameters(), lr = 1e-1)\n",
        "y_hat1, y_hat2 = model1(X)\n",
        "loss1 = F.binary_cross_entropy(y_hat1, y)\n",
        "loss2 = F.binary_cross_entropy(y_hat2, y)\n",
        "optimizer.zero_grad()\n",
        "loss1.backward()\n",
        "loss2.backward()"
      ],
      "metadata": {
        "id": "D9nOudns_2XS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weight_norm1 = []\n",
        "weight_norm2 = []\n",
        "\n",
        "for i in range(0, 35, 2):\n",
        "    w_norm1 = torch.sum(torch.abs(model1.linear1[i].weight.grad))\n",
        "    w_norm2 = torch.sum(torch.abs(model1.linear2[i].weight.grad))\n",
        "\n",
        "    weight_norm1 += [w_norm1]\n",
        "    weight_norm2 += [w_norm2]"
      ],
      "metadata": {
        "id": "RZcUCR2pBar-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(0,10,2), weight_norm1[0:5], 'ro', label = 'Sigmoid')\n",
        "plt.plot(range(0,10,2), weight_norm2[0:5], 'bo', label = 'LeLU')\n",
        "\n",
        "plt.xlabel('Layer_Index')\n",
        "plt.ylabel('Weighy_Matrix_Norm')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(weight_norm1)\n",
        "print(weight_norm2)"
      ],
      "metadata": {
        "id": "OhDTEqp9F7nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습과정 관찰.\n",
        "\n",
        "Sigmoid를 활성화함수를 사용하면 vanishing gradient 현상이 발생하지만 ReLU를 사용하면 극복할 수 있는 적당한 깊이의 모델을 사용해 가정1을 실험해본다.\n",
        "\\\n",
        "\\\n",
        "! 초기에 아주깊은 신경망을 생성한후 실험했을땐 Sigmoid와 ReLU모두 Loss값 0.6 부근에서 학습이 정체되어 가정1이 틀렸다고 생각했지만 ReLU가 vanishing gradient를 완전히 극복할 수 없음을 알고 적당한 깊이로 실험한 결과 Sigmoid를 사용한 모델은 학습이 정체되는 반면 ReLU를 사용하면 학습이 잘이루어지는것을 관찰했다."
      ],
      "metadata": {
        "id": "u8pWFZ71gQgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Sequential(nn.Linear(2, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                     nn.Linear(10, 1), nn.Sigmoid())\n",
        "\n",
        "        self.linear2 = nn.Sequential(nn.Linear(2, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 10), nn.ReLU(),\n",
        "                                     nn.Linear(10, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x1 = self.linear1(x)\n",
        "        x2 = self.linear2(x)\n",
        "\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "sWlFHor2gtI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-1\n",
        "EPOCH = 1000\n",
        "\n",
        "model2 = Model2()\n",
        "\n",
        "optimizer = optim.SGD(model2.parameters(), lr=LR)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "loss_history1 =[]\n",
        "loss_history2 =[]\n",
        "\n",
        "model2.train()\n",
        "for ep in range(EPOCH):\n",
        "\n",
        "    y_hat1, y_hat2 = model2(X)\n",
        "\n",
        "    loss1 = F.binary_cross_entropy(y_hat1, y)\n",
        "    loss2 = F.binary_cross_entropy(y_hat2, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss1.backward()\n",
        "    loss2.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history1 += [loss1.item()]\n",
        "    loss_history2 += [loss2.item()]\n",
        "\n",
        "plt.plot(range(1,EPOCH+1),loss_history1, label = 'Sigmoid')\n",
        "plt.plot(range(1,EPOCH+1),loss_history2, label = 'ReLU')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bk9KrTwJEL1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가정2-실험2"
      ],
      "metadata": {
        "id": "IW0WGbIcnKNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습정체를 일으키고 gradient를 확인하기 위한 모델을 생성"
      ],
      "metadata": {
        "id": "WPoMeTu1pqfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model3(nn.Module()):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear = nn.sequential(nn.Linear(2, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10), nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 1), nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Zj7NyTwkoJgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR = 1e-1\n",
        "EPOCH = 1000\n",
        "\n",
        "model3 = Model3()\n",
        "\n",
        "optimizer = optim.SGD(model3.parameters(), lr=LR)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "loss_history3 =[]\n",
        "\n",
        "model2.train()\n",
        "for ep in range(EPOCH):\n",
        "\n",
        "    y_hat3 = model3(X)\n",
        "\n",
        "    loss3 = F.binary_cross_entropy(y_hat3, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss3.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_history3 += [loss3.item()]\n",
        "\n",
        "plt.plot(range(1,EPOCH+1),loss_history3, label = 'Sigmoid')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cgdOFkq8pk9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x1_test=torch.linspace(-10,10,30) # case 1\n",
        "# x2_test=torch.linspace(-10,10,30) # case 1\n",
        "x1_test = torch.linspace(-10,10,30)\n",
        "x2_test = torch.linspace(-10,10,30)\n",
        "\n",
        "X1_test, X2_test = torch.meshgrid(x1_test, x2_test)\n",
        "# print(X1_test.shape)\n",
        "\n",
        "X_test = torch.cat([X1_test.unsqueeze(dim=2), X2_test.unsqueeze(dim=2)], dim=2)\n",
        "# print(X_test)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(X_test)\n",
        "\n",
        "Y_hat = y_hat.squeeze()\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 9]) # figsize=[가로, 세로]\n",
        "ax = plt.axes(projection=\"3d\")\n",
        "ax.view_init(elev=25,azim=-140)\n",
        "ax.plot_surface(X1_test,X2_test, Y_hat.numpy(), cmap=\"viridis\", alpha=0.2)\n",
        "plt.plot(class1_data[:,0],class1_data[:,1],class1_label.squeeze(),'bo')\n",
        "plt.plot(class2_data[:,0],class2_data[:,1],class2_label.squeeze(),'ro')\n",
        "# plt.plot(class3_data[:,0],class3_data[:,1],class3_label.squeeze(),'bo')\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")"
      ],
      "metadata": {
        "id": "xvAFEsNJKGPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}