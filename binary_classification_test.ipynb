{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "7oSt8g7GzHMO"
      ],
      "authorship_tag": "ABX9TyOTxnJeLoWDYZixVPvC/+OP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/binary_classification_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델의 depth와 활성함수의 중첩, 그에 따른 비선형성 증가의 의미.\n",
        "\n",
        "딥러닝 모델의 깊어질수록 활성함수가 중첩되고 모델전체의 비선형성이 증가하게 된다. 비선형성이 증가함에따라 모델이 그리는 곡면은 더 복잡해지게 되므로 SGD같이 local minimum을 탈출할 수단이 없는 optimizer는 local minimum에 빠질 확률이 커지게 될 것 이다. 이번 프로젝트에서는 이러한 현상을 시각화해 깊이 이해하고 이를 극복하기위한 여러 optimize를 활용해 보고자 한다."
      ],
      "metadata": {
        "id": "kfp1WidqwT8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data 생성"
      ],
      "metadata": {
        "id": "7oSt8g7GzHMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "j6dQVVruCJup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSQnPan6AoTN"
      },
      "outputs": [],
      "source": [
        "# case 1 키, 몸무게\n",
        "N=20\n",
        "random0=torch.randn(int(N/2),1)\n",
        "random5=torch.randn(int(N/2),1)+8\n",
        "\n",
        "class1_data=torch.hstack([random0,random5])\n",
        "class2_data=torch.hstack([random5,random0])\n",
        "# class3_data=torch.hstack([random5,random0+8])\n",
        "\n",
        "class1_label=torch.ones(int(N/2),1)\n",
        "class2_label=torch.zeros(int(N/2),1)\n",
        "class3_label=torch.ones(int(N/2),1)\n",
        "\n",
        "X=torch.vstack([class1_data,class2_data])\n",
        "y=torch.vstack([class1_label,class2_label])\n",
        "\n",
        "# X=torch.vstack([class1_data,class2_data,class3_data])\n",
        "# y=torch.vstack([class1_label,class2_label,class3_label])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(class1_data[:,0],class1_data[:,1],'bo')\n",
        "plt.plot(class2_data[:,0],class2_data[:,1],'ro')\n",
        "# plt.plot(class3_data[:,0],class3_data[:,1],'bo')\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.grid()"
      ],
      "metadata": {
        "id": "GFZpfnNVCWwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델생성과 학습"
      ],
      "metadata": {
        "id": "wpcl0qvGvoyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "비슷한 수의 parameter(각각 300개, 330개)을 갖지만 깊이가 다른(각각 2층, 5층) model 2개를 생성한다."
      ],
      "metadata": {
        "id": "ji_n0cVG55ZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = nn.Sequential(nn.Linear(2, 100),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(100, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "        self.linear2 = nn.Sequential(nn.Linear(2, 10),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 10),\n",
        "                                    nn.Sigmoid(),\n",
        "                                    nn.Linear(10, 1),\n",
        "                                    nn.Sigmoid()\n",
        "                                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.linear1(x)\n",
        "        x2 = self.linear2(x)\n",
        "\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "OS8Q_N6jCcLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 모델을 SGD로 학습시키고 진행에 따른 Loss값 변화를 그래프로 비교한다."
      ],
      "metadata": {
        "id": "HM-m1vmV6Wqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "\n",
        "LR = 1e-1\n",
        "EPOC = 100\n",
        "\n",
        "# optimizer = optim.Adam(model.parameters(), lr = LR)\n",
        "optimizer = optim.SGD(model.parameters(), lr = LR)\n",
        "\n",
        "loss1_history = []\n",
        "loss2_history = [] # 추가\n",
        "\n",
        "model.train()\n",
        "for ep in range(EPOC):\n",
        "    y1_hat, y2_hat = model(X)\n",
        "\n",
        "    loss1 = F.binary_cross_entropy(y1_hat, y)\n",
        "    loss2 = F.binary_cross_entropy(y2_hat, y) # 추가\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss1.backward()\n",
        "    loss2.backward() # 추가\n",
        "    optimizer.step()\n",
        "\n",
        "    loss1_history += [loss1.item()]\n",
        "    loss2_history += [loss2.item()] # 추가\n",
        "\n",
        "plt.plot(range(1,EPOC+1), loss1_history)\n",
        "plt.plot(range(1,EPOC+1), loss2_history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "metadata": {
        "id": "bk9KrTwJEL1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yf-eegyd6-Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x1_test=torch.linspace(-10,10,30) # case 1\n",
        "# x2_test=torch.linspace(-10,10,30) # case 1\n",
        "x1_test = torch.linspace(-10,10,30)\n",
        "x2_test = torch.linspace(-10,10,30)\n",
        "\n",
        "X1_test, X2_test = torch.meshgrid(x1_test, x2_test)\n",
        "# print(X1_test.shape)\n",
        "\n",
        "X_test = torch.cat([X1_test.unsqueeze(dim=2), X2_test.unsqueeze(dim=2)], dim=2)\n",
        "# print(X_test)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_hat = model(X_test)\n",
        "\n",
        "Y_hat = y_hat.squeeze()\n",
        "\n",
        "\n",
        "plt.figure(figsize=[10, 9]) # figsize=[가로, 세로]\n",
        "ax = plt.axes(projection=\"3d\")\n",
        "ax.view_init(elev=25,azim=-140)\n",
        "ax.plot_surface(X1_test,X2_test, Y_hat.numpy(), cmap=\"viridis\", alpha=0.2)\n",
        "plt.plot(class1_data[:,0],class1_data[:,1],class1_label.squeeze(),'bo')\n",
        "plt.plot(class2_data[:,0],class2_data[:,1],class2_label.squeeze(),'ro')\n",
        "# plt.plot(class3_data[:,0],class3_data[:,1],class3_label.squeeze(),'bo')\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")"
      ],
      "metadata": {
        "id": "xvAFEsNJKGPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}