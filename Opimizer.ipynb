{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "K8c47feGi5-e",
        "J6rkeUIPlP0e",
        "z6nRbIM239ft",
        "qyG6VDmAx8iy",
        "JJUIbIUM9ViX"
      ],
      "authorship_tag": "ABX9TyPkJS5yCGEJ1/7iZ7UIlZiG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiwoong2/deeplearning/blob/main/Opimizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdl35NzCijt4"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD(Stochastic Gradient Decent)"
      ],
      "metadata": {
        "id": "K8c47feGi5-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률적 경사하강법: 전체 데이터셋을 한번에 학습 시키는 것은 많은 컴퓨팅 자원을 필요로 하는 일 이기 때문에 데이터셋을 분할해서 학습을 진행하는 것을 확률적 경사하강법이라고 한다. 하지만 오차함수의 상수인 데이터가 계속해서 변하게 되므로 하강궤적이 불안정하게 된다.\n",
        "\n",
        "분할한 데이터를 학습해서 전체 데이터셋을 모두 학습 완료하는것을 에폭이라고 한다."
      ],
      "metadata": {
        "id": "0a1iJ9Nr_Z02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key]"
      ],
      "metadata": {
        "id": "InXceBtViyMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Momentum"
      ],
      "metadata": {
        "id": "J6rkeUIPlP0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "물리학의 관선에서 아이디어를 얻어 적용한 기법으로 Gradient Decent에서 방향과 학습률을 적용했다면 Momentum에서는 관성을 추가하게 된다. Gradient Decent는 초기값에 따라 Local minimum이나 plateau에서 학습이 진행돼지 않는 문제가 있는 반해 Momentum은 관성을 이용해 빠져나올 수 있게 된다.\n",
        "또 매개변수간 변화율으 차이가 크게 되는 경우 Gradient Decent의 궤적이 진동하며 매우 비효율적인 동선을 보이는데 Momentum을 사용하게 되면 이런 문제점도 약간 개선된다.\n",
        "\n",
        "점화식은\n",
        "\n",
        "$V_{n} = \\alpha V_{n-1} - \\eta \\bigtriangledown f(x_n) $\n",
        "\n",
        "$V_{-1} = 0 $\n",
        "\n",
        "$X_{n+1} = X_n + V_n$\n",
        "\n",
        "물리계에 비유하면 $\\alpha V_{n-1}$가 관성이 되고, $\\alpha$ 는 관성계수가 된다."
      ],
      "metadata": {
        "id": "HAEC7G1G_e42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Momentum:\n",
        "\n",
        "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]\n",
        "            params[key] += self.v[key]"
      ],
      "metadata": {
        "id": "ZI3-IUEkoE8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAG(Nesteou Accelated Gradient)"
      ],
      "metadata": {
        "id": "z6nRbIM239ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum은 현재위치에서 관성과 Gradient의 반대방향을 합한다.\n",
        "NAG는 Momentume을 공격적으로 변환한다. 현재위치에서 관성과 관성방향으로 움직인 후의 Gradient의 반대방향을 합하게 된다.\n",
        "\n",
        "점화식은\n",
        "\n",
        "$V_{n} = \\alpha V_{n-1} - \\eta \\bigtriangledown f(x_n + \\alpha V_{n-1}) $\n",
        "\n",
        "$V_{-1} = 0 $\n",
        "\n",
        "$X_{n+1} = X_n + V_n$\n",
        "\n",
        "하지만 이 NAG는 $X_n$이 아닌 다른점에서 Gradient를 구하기 때문에 오차역전파를 사용하는 신경망에서는 구현하기 적합하지 않다. 대신 Bengio의 근사적 접근을 사용할 수 있다.\n",
        "\n",
        "Bengio의 근사적 접근은 $X_n$의 궤적 대신 $X_n'$의 궤적으로 Gradient Dsent를 진행한다. 그러면 파라미터를 수정하지 않아도 되고 최종적으로는 관성이 줄어들며 $X_n$ 과 $X_n'$이 서로 근하하게 되므로 신경망에서 사용하기 적합하게 되는 것 이다.\n",
        "\n",
        "NAG에서 Bengio의 근사적방법을 적용하는 방법은 노트 참고."
      ],
      "metadata": {
        "id": "M4L5qaqP_iuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Nesterov:\n",
        "\n",
        "    def __init__(self, lr = 0.01, momentum = 0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.mometum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]"
      ],
      "metadata": {
        "id": "zraTM82S39I0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaGrad\n"
      ],
      "metadata": {
        "id": "qyG6VDmAx8iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGrad는 일정한 learning rate를 사용하지 않고 각 변수마다 그리고 매 step마다 다른 learning rate를 사용한다.\n",
        "\n",
        "점화식\n",
        "\n",
        "$h_n = h_{n-1} + \\bigtriangledown f(x_n)$,\n",
        "\n",
        "$h_{-1} = 0$\n",
        "\n",
        "$x_{n+1} = x_n - \\eta \\frac{1}{\\sqrt{h_n}} \\odot \\bigtriangledown f(x_n) $\n",
        "\n",
        "점화식을 보면 $h_n$은 학습이 진행될수록 계속해서 커지며, 학습중에 큰 gradient값이 나오면 후에 계속해서 영향을 끼칭을 알 수 있다. 이는 큰 변화를 겪은 변수는 이미 최적에 가까워졌고 작은 변화를 겪은 변수는 최적에서 아직 멀다고 생각하기 때문이다."
      ],
      "metadata": {
        "id": "VhBdK4qt_pxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaGrad:\n",
        "\n",
        "    def __init__(self, lr = 0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key] # hadamard product.\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7) # 1e-7은 erorr방지."
      ],
      "metadata": {
        "id": "Mi2p18CWx7Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test, 문제 검산"
      ],
      "metadata": {
        "id": "1rgxh8Kb3t3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ASA = AdaGrad(1)\n",
        "params = {'x':1, 'y':2}\n",
        "grads = {'x':2, 'y':1}"
      ],
      "metadata": {
        "id": "DJ18qM_33srX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASA.update(params, grads)"
      ],
      "metadata": {
        "id": "2yTW7C-v4GPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASA.h"
      ],
      "metadata": {
        "id": "D9dpo93z4PP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "BAqjPROW4eAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSProp\n"
      ],
      "metadata": {
        "id": "JJUIbIUM9ViX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaGrad는 스텝이 많이 진행되면 누적치 $h_n$이 너무 커져서 학습률이 작이져 학습이 거의 되지 않는 문제가 있다. 이를 보완하기 위해 RMSProp은 이전 누적치와 현재 gradient의 좌표별 제곱의 가중치 평균을 생각한다. 이는 AdaGrad보다 최근 값을 더 반영하게 해준다.\n",
        "\n",
        "점화식은\n",
        "\n",
        "$h_n = \\gamma h_{n-1} + (1-\\gamma)\\bigtriangledown f(x_n),$\n",
        "\n",
        "$h_{-1} = 0$\n",
        "\n",
        "$x_{n+1} = x_n - \\eta \\frac{1}{\\sqrt{h_n}}\\odot\\bigtriangledown f(x_n)$\n",
        "\n",
        "$\\gamma$는 forgetting factor또는 decay rate이라고 하며 클수록 과거가 중요하고 작을수록 현재가 중요하다."
      ],
      "metadata": {
        "id": "6Uo13OcT_s_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSprop:\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate=0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = np.zeros_like(val)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1-self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)"
      ],
      "metadata": {
        "id": "F2OKpt52-4l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test, 문제 검산"
      ],
      "metadata": {
        "id": "mTuNHySxBwO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zelda=RMSprop(1., 3/4)\n",
        "params = {'x':1., 'y':2.}\n",
        "grads = {'x':2., 'y':1.}"
      ],
      "metadata": {
        "id": "VNd8gHhrBI8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zelda.update(params, grads)"
      ],
      "metadata": {
        "id": "OUzuW2FsCVsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zelda.h"
      ],
      "metadata": {
        "id": "BeYjMygWCkgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "tsQSyTsLCom9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam"
      ],
      "metadata": {
        "id": "XI7fGEtJ0SWI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Momentum과 BMSProp 두가지 방식을 혼합한 형식의 optimizer. 따라서 복잡하지만 현재 가장 많이 사용되는 optimizer이다.\n",
        "\n",
        "Momentum을 변형하여\n",
        "\n",
        "$m_n = \\beta_1 m_{n-1} + (1-\\beta_1) \\bigtriangledown f(x_n)$\n",
        "\n",
        "$m_{-1} = 0$\n",
        "\n",
        "Adagrad를 변형하여\n",
        "\n",
        "$v_n = \\beta_2 v_{n-1} + (1-\\beta_2)\\bigtriangledown f(x_n) \\odot \\bigtriangledown f(x_n)$\n",
        "\n",
        "$v_{-1} = 0$\n",
        "\n",
        "을 생각한다.\n",
        "\n",
        "초기 값이 0이기 떄문에 $\\beta_1, \\beta_2$ 가 1에 가까우면 0으로 편향되기 떄문에 이를 보정하기위해 $\\hat{m}_n = \\frac{m_n}{1 - \\beta _1 ^ {n+1}}, v_n = \\frac{v_n}{1- \\beta _2 ^{n+1}}$ 을 생각한다.\n",
        "\n",
        "점의 이동 점화식은\n",
        "\n",
        "$x_{n+1} = x_n - \\eta \\frac{1}{\\sqrt{\\hat{v}_n}} \\odot m_n$"
      ],
      "metadata": {
        "id": "4AH8jhNR0WDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Adam:\n",
        "\n",
        "    def __init__(self, lr = 0.001, beta1 = 0.9, beta2 = 0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = np.zeros_like(val)\n",
        "                self.v[key] = np.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * np.sqrt(1.0 - self.beta2 ** self.iter) / (1.0 - self.beta1 ** self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)"
      ],
      "metadata": {
        "id": "KYex9QVA4F3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test, 문제검산"
      ],
      "metadata": {
        "id": "xPn5hcLJ7btY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eve = Adam(1. ,1/2, 1/2)\n",
        "\n",
        "params = {'x' : 1., 'y' : 2.}\n",
        "grads = {'x' : 2., 'y' : 1.}\n",
        "eve.update(params, grads)"
      ],
      "metadata": {
        "id": "UYaU6hCR5kh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eve.m"
      ],
      "metadata": {
        "id": "p4z3BvQ-77Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eve.v"
      ],
      "metadata": {
        "id": "HpPsZdtG8A6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "ZBvtCmhu8I4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}